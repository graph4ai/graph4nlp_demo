{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934b74e1",
   "metadata": {},
   "source": [
    "# AAAIâ€™22 DLG4NLP Tutorial Demo: Semantic Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346fd68",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c3b2e",
   "metadata": {},
   "source": [
    "In this tutorial demo, we will use the Graph4NLP library to build a GNN-based semantic parsing model. The model consists of \n",
    "- graph construction module (e.g., node embedding based dynamic graph)\n",
    "- graph embedding module (e.g., Bi-Sep GAT)\n",
    "- prediction module (e.g., RNN decoder with attention, copy and coverage mechanisms)\n",
    "\n",
    "We will use the built-in Graph2Seq model APIs to build the model, and evaluate it on the Jobs dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac004ea7",
   "metadata": {},
   "source": [
    "### Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de88301",
   "metadata": {},
   "source": [
    "Please follow the instructions [here](https://github.com/graph4ai/graph4nlp_demo#environment-setup) to set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086ca306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from graph4nlp.pytorch.datasets.jobs import JobsDataset\n",
    "from graph4nlp.pytorch.models.graph2seq import Graph2Seq\n",
    "from graph4nlp.pytorch.models.graph2seq_loss import Graph2SeqLoss\n",
    "from graph4nlp.pytorch.modules.graph_construction import *\n",
    "from graph4nlp.pytorch.modules.evaluation.base import EvaluationMetricBase\n",
    "from graph4nlp.pytorch.modules.utils.config_utils import update_values\n",
    "from graph4nlp.pytorch.modules.utils.copy_utils import prepare_ext_vocab\n",
    "from graph4nlp.pytorch.modules.config import get_basic_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b0d6b",
   "metadata": {},
   "source": [
    "### Build the model handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b7543",
   "metadata": {},
   "source": [
    "Let's build a model handler which will do a bunch of things including setting up dataloader, model, optimizer, evaluation metrics, train/val/test loops, and so on.\n",
    "\n",
    "We will call the Graph2Seq model API which implements a GNN-based encoder and LSTM-based decoder.\n",
    "\n",
    "When setting up the dataloader, users will need to call the dataset API which will preprocess the data, e.g., calling the graph construction module, building the vocabulary, tensorizing the data. Users will need to specify the graph construction type when calling the dataset API.\n",
    "\n",
    "Users can build their customized dataset APIs by inheriting our low-level dataset APIs. We provide low-level dataset APIs to support various scenarios (e.g., `Text2Label`, `Sequence2Labeling`, `Text2Text`, `Text2Tree`, `DoubleText2Text`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1637d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jobs:\n",
    "    def __init__(self, opt):\n",
    "        super(Jobs, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.use_copy = self.opt[\"decoder_args\"][\"rnn_decoder_share\"][\"use_copy\"]\n",
    "        self.use_coverage = self.opt[\"decoder_args\"][\"rnn_decoder_share\"][\"use_coverage\"]\n",
    "        self._build_device(self.opt)\n",
    "        self._build_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "        self._build_evaluation()\n",
    "        self._build_loss_function()\n",
    "\n",
    "    def _build_device(self, opt):\n",
    "        seed = opt[\"seed\"]\n",
    "        np.random.seed(seed)\n",
    "        if opt[\"use_gpu\"] != 0 and torch.cuda.is_available():\n",
    "            print(\"[ Using CUDA ]\")\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            from torch.backends import cudnn\n",
    "\n",
    "            cudnn.benchmark = True\n",
    "            device = torch.device(\"cuda\" if opt[\"gpu\"] < 0 else \"cuda:%d\" % opt[\"gpu\"])\n",
    "        else:\n",
    "            print(\"[ Using CPU ]\")\n",
    "            device = torch.device(\"cpu\")\n",
    "        self.device = device\n",
    "\n",
    "    def _build_dataloader(self):\n",
    "        dataset = JobsDataset(\n",
    "            root_dir=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"root_dir\"],\n",
    "            pretrained_word_emb_name=self.opt[\"pretrained_word_emb_name\"],\n",
    "            merge_strategy=self.opt[\"graph_construction_args\"][\"graph_construction_private\"][\n",
    "                \"merge_strategy\"\n",
    "            ],\n",
    "            edge_strategy=self.opt[\"graph_construction_args\"][\"graph_construction_private\"][\n",
    "                \"edge_strategy\"\n",
    "            ],\n",
    "            seed=self.opt[\"seed\"],\n",
    "            word_emb_size=self.opt[\"word_emb_size\"],\n",
    "            share_vocab=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "                \"share_vocab\"\n",
    "            ],\n",
    "            graph_name=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "                \"graph_name\"\n",
    "            ],\n",
    "            dynamic_init_graph_name=self.opt[\"graph_construction_args\"][\n",
    "                \"graph_construction_private\"\n",
    "            ].get(\"dynamic_init_graph_type\", None),\n",
    "            topology_subdir=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "                \"topology_subdir\"\n",
    "            ],\n",
    "            thread_number=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\n",
    "                \"thread_number\"\n",
    "            ],\n",
    "            port=self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"port\"],\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            dataset.train,\n",
    "            batch_size=self.opt[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=self.opt[\"num_workers\"],\n",
    "            collate_fn=dataset.collate_fn,\n",
    "        )\n",
    "        self.test_dataloader = DataLoader(\n",
    "            dataset.test,\n",
    "            batch_size=self.opt[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=self.opt[\"num_workers\"],\n",
    "            collate_fn=dataset.collate_fn,\n",
    "        )\n",
    "        self.vocab = dataset.vocab_model\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.model = Graph2Seq.from_args(self.opt, self.vocab).to(self.device)\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = optim.Adam(parameters, lr=self.opt[\"learning_rate\"])\n",
    "\n",
    "    def _build_evaluation(self):\n",
    "        self.metrics = [ExpressionAccuracy()]\n",
    "\n",
    "    def _build_loss_function(self):\n",
    "        self.loss = Graph2SeqLoss(\n",
    "            ignore_index=self.vocab.out_word_vocab.PAD,\n",
    "            use_coverage=self.use_coverage,\n",
    "            coverage_weight=0.3,\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        max_score = -1\n",
    "        self._best_epoch = -1\n",
    "        for epoch in range(200):\n",
    "            self.model.train()\n",
    "            self.train_epoch(epoch, split=\"train\")\n",
    "            self._adjust_lr(epoch)\n",
    "            if epoch >= 0:\n",
    "                score = self.evaluate(split=\"test\")\n",
    "                if score >= max_score:\n",
    "                    print(\"Best model saved, epoch {}\".format(epoch))\n",
    "                    self.model.save_checkpoint(self.opt[\"checkpoint_save_path\"], \"best.pt\")\n",
    "                    self._best_epoch = epoch\n",
    "                max_score = max(max_score, score)\n",
    "            if epoch >= 30 and self._stop_condition(epoch):\n",
    "                break\n",
    "        return max_score\n",
    "\n",
    "    def _stop_condition(self, epoch, patience=20):\n",
    "        return epoch > patience + self._best_epoch\n",
    "\n",
    "    def _adjust_lr(self, epoch):\n",
    "        def set_lr(optimizer, decay_factor):\n",
    "            for group in optimizer.param_groups:\n",
    "                group[\"lr\"] = group[\"lr\"] * decay_factor\n",
    "\n",
    "        epoch_diff = epoch - self.opt[\"lr_start_decay_epoch\"]\n",
    "        if epoch_diff >= 0 and epoch_diff % self.opt[\"lr_decay_per_epoch\"] == 0:\n",
    "            if self.opt[\"learning_rate\"] > self.opt[\"min_lr\"]:\n",
    "                set_lr(self.optimizer, self.opt[\"lr_decay_rate\"])\n",
    "                self.opt[\"learning_rate\"] = self.opt[\"learning_rate\"] * self.opt[\"lr_decay_rate\"]\n",
    "                print(\"Learning rate adjusted: {:.5f}\".format(self.opt[\"learning_rate\"]))\n",
    "\n",
    "    def train_epoch(self, epoch, split=\"train\"):\n",
    "        assert split in [\"train\"]\n",
    "        print(\"Start training in split {}, Epoch: {}\".format(split, epoch))\n",
    "        loss_collect = []\n",
    "        dataloader = self.train_dataloader\n",
    "        step_all_train = len(dataloader)\n",
    "        for step, data in enumerate(dataloader):\n",
    "            graph, tgt, gt_str = data[\"graph_data\"], data[\"tgt_seq\"], data[\"output_str\"]\n",
    "            graph = graph.to(self.device)\n",
    "            tgt = tgt.to(self.device)\n",
    "            oov_dict = None\n",
    "            if self.use_copy:\n",
    "                oov_dict, tgt = prepare_ext_vocab(\n",
    "                    graph, self.vocab, gt_str=gt_str, device=self.device\n",
    "                )\n",
    "\n",
    "            prob, enc_attn_weights, coverage_vectors = self.model(graph, tgt, oov_dict=oov_dict)\n",
    "            loss = self.loss(\n",
    "                logits=prob,\n",
    "                label=tgt,\n",
    "                enc_attn_weights=enc_attn_weights,\n",
    "                coverage_vectors=coverage_vectors,\n",
    "            )\n",
    "            loss_collect.append(loss.item())\n",
    "            if step % self.opt[\"loss_display_step\"] == 0 and step != 0:\n",
    "                print(\n",
    "                    \"Epoch {}: [{} / {}] loss: {:.3f}\".format(\n",
    "                        epoch, step, step_all_train, np.mean(loss_collect)\n",
    "                    )\n",
    "                )\n",
    "                loss_collect = []\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def evaluate(self, split=\"val\"):\n",
    "        self.model.eval()\n",
    "        pred_collect = []\n",
    "        gt_collect = []\n",
    "        assert split in [\"test\"]\n",
    "        dataloader = self.test_dataloader\n",
    "        for data in dataloader:\n",
    "            graph, gt_str = data[\"graph_data\"], data[\"output_str\"]\n",
    "            graph = graph.to(self.device)\n",
    "            if self.use_copy:\n",
    "                oov_dict = prepare_ext_vocab(\n",
    "                    batch_graph=graph, vocab=self.vocab, device=self.device\n",
    "                )\n",
    "                ref_dict = oov_dict\n",
    "            else:\n",
    "                oov_dict = None\n",
    "                ref_dict = self.vocab.out_word_vocab\n",
    "\n",
    "            prob, _, _ = self.model(graph, oov_dict=oov_dict)\n",
    "            pred = prob.argmax(dim=-1)\n",
    "\n",
    "            pred_str = wordid2str(pred.detach().cpu(), ref_dict)\n",
    "            pred_collect.extend(pred_str)\n",
    "            gt_collect.extend(gt_str)\n",
    "\n",
    "        score = self.metrics[0].calculate_scores(ground_truth=gt_collect, predict=pred_collect)\n",
    "        print(\"Evaluation accuracy in `{}` split: {:.3f}\".format(split, score))\n",
    "        return score\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def translate(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        pred_collect = []\n",
    "        gt_collect = []\n",
    "        dataloader = self.test_dataloader\n",
    "        for data in dataloader:\n",
    "            graph, gt_str = data[\"graph_data\"], data[\"output_str\"]\n",
    "            graph = graph.to(self.device)\n",
    "            if self.use_copy:\n",
    "                oov_dict = prepare_ext_vocab(\n",
    "                    batch_graph=graph, vocab=self.vocab, device=self.device\n",
    "                )\n",
    "                ref_dict = oov_dict\n",
    "            else:\n",
    "                oov_dict = None\n",
    "                ref_dict = self.vocab.out_word_vocab\n",
    "\n",
    "            pred = self.model.translate(batch_graph=graph, oov_dict=oov_dict, beam_size=4, topk=1)\n",
    "\n",
    "            pred_ids = pred[:, 0, :]  # we just use the top-1\n",
    "\n",
    "            pred_str = wordid2str(pred_ids.detach().cpu(), ref_dict)\n",
    "\n",
    "            pred_collect.extend(pred_str)\n",
    "            gt_collect.extend(gt_str)\n",
    "\n",
    "        score = self.metrics[0].calculate_scores(ground_truth=gt_collect, predict=pred_collect)\n",
    "        print(\"Evaluation accuracy in `{}` split: {:.3f}\".format(\"test\", score))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac6e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.vocab as vocab\n",
    "from graph4nlp.pytorch.modules.utils.vocab_utils import Vocab\n",
    "\n",
    "\n",
    "class ExpressionAccuracy(EvaluationMetricBase):\n",
    "    def __init__(self):\n",
    "        super(ExpressionAccuracy, self).__init__()\n",
    "\n",
    "    def calculate_scores(self, ground_truth, predict):\n",
    "        correct = 0\n",
    "        assert len(ground_truth) == len(predict)\n",
    "        for gt, pred in zip(ground_truth, predict):\n",
    "            if gt == pred:\n",
    "                correct += 1.0\n",
    "        return correct / len(ground_truth)\n",
    "\n",
    "def wordid2str(word_ids, vocab: Vocab):\n",
    "    ret = []\n",
    "    assert len(word_ids.shape) == 2, print(word_ids.shape)\n",
    "    for i in range(word_ids.shape[0]):\n",
    "        id_list = word_ids[i, :]\n",
    "        ret_inst = []\n",
    "        for j in range(id_list.shape[0]):\n",
    "            if id_list[j] == vocab.EOS:\n",
    "                break\n",
    "            token = vocab.getWord(id_list[j])\n",
    "            ret_inst.append(token)\n",
    "        ret.append(\" \".join(ret_inst))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf198e",
   "metadata": {},
   "source": [
    "### Set up the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a702636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config setup\n",
    "config_file = '../config/jobs/gat_bi_sep_dynamic_node_emb_v2.yaml'\n",
    "config = yaml.load(open(config_file, 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "opt = get_basic_args(graph_construction_name=config[\"graph_construction_name\"],\n",
    "                              graph_embedding_name=config[\"graph_embedding_name\"],\n",
    "                              decoder_name=config[\"decoder_name\"])\n",
    "update_values(to_args=opt, from_args_list=[config, config[\"other_args\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985bc54a",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb15c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using CPU ]\n",
      "Start training in split train, Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/opt/anaconda3/envs/graph4nlp_0.5.5/lib/python3.8/site-packages/graph4nlp-0.5.5-py3.8.egg/graph4nlp/pytorch/modules/graph_embedding_learning/gat.py:259: UserWarning: The residual option must be False when num_heads > 1\n",
      "  warnings.warn(\"The residual option must be False when num_heads > 1\")\n",
      "/Users/hugo/opt/anaconda3/envs/graph4nlp_0.5.5/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [10 / 21] loss: 3.588\n",
      "Epoch 0: [20 / 21] loss: 2.292\n",
      "Evaluation accuracy in `test` split: 0.000\n",
      "Best model saved, epoch 0\n",
      "Start training in split train, Epoch: 1\n",
      "Epoch 1: [10 / 21] loss: 1.653\n",
      "Epoch 1: [20 / 21] loss: 1.343\n",
      "Evaluation accuracy in `test` split: 0.000\n",
      "Best model saved, epoch 1\n",
      "Start training in split train, Epoch: 2\n",
      "Epoch 2: [10 / 21] loss: 1.192\n",
      "Epoch 2: [20 / 21] loss: 1.048\n",
      "Evaluation accuracy in `test` split: 0.143\n",
      "Best model saved, epoch 2\n",
      "Start training in split train, Epoch: 3\n",
      "Epoch 3: [10 / 21] loss: 1.000\n",
      "Epoch 3: [20 / 21] loss: 0.953\n",
      "Evaluation accuracy in `test` split: 0.071\n",
      "Start training in split train, Epoch: 4\n",
      "Epoch 4: [10 / 21] loss: 0.895\n",
      "Epoch 4: [20 / 21] loss: 0.925\n",
      "Evaluation accuracy in `test` split: 0.136\n",
      "Start training in split train, Epoch: 5\n",
      "Epoch 5: [10 / 21] loss: 0.857\n",
      "Epoch 5: [20 / 21] loss: 0.821\n",
      "Evaluation accuracy in `test` split: 0.036\n",
      "Start training in split train, Epoch: 6\n",
      "Epoch 6: [10 / 21] loss: 0.806\n",
      "Epoch 6: [20 / 21] loss: 0.780\n",
      "Evaluation accuracy in `test` split: 0.093\n",
      "Start training in split train, Epoch: 7\n",
      "Epoch 7: [10 / 21] loss: 0.747\n",
      "Epoch 7: [20 / 21] loss: 0.735\n",
      "Evaluation accuracy in `test` split: 0.336\n",
      "Best model saved, epoch 7\n",
      "Start training in split train, Epoch: 8\n",
      "Epoch 8: [10 / 21] loss: 0.701\n",
      "Epoch 8: [20 / 21] loss: 0.729\n",
      "Evaluation accuracy in `test` split: 0.286\n",
      "Start training in split train, Epoch: 9\n",
      "Epoch 9: [10 / 21] loss: 0.691\n",
      "Epoch 9: [20 / 21] loss: 0.679\n",
      "Evaluation accuracy in `test` split: 0.057\n",
      "Start training in split train, Epoch: 10\n",
      "Epoch 10: [10 / 21] loss: 0.672\n",
      "Epoch 10: [20 / 21] loss: 0.632\n",
      "Evaluation accuracy in `test` split: 0.329\n",
      "Start training in split train, Epoch: 11\n",
      "Epoch 11: [10 / 21] loss: 0.625\n",
      "Epoch 11: [20 / 21] loss: 0.662\n",
      "Evaluation accuracy in `test` split: 0.436\n",
      "Best model saved, epoch 11\n",
      "Start training in split train, Epoch: 12\n",
      "Epoch 12: [10 / 21] loss: 0.599\n",
      "Epoch 12: [20 / 21] loss: 0.593\n",
      "Evaluation accuracy in `test` split: 0.493\n",
      "Best model saved, epoch 12\n",
      "Start training in split train, Epoch: 13\n",
      "Epoch 13: [10 / 21] loss: 0.597\n",
      "Epoch 13: [20 / 21] loss: 0.591\n",
      "Evaluation accuracy in `test` split: 0.529\n",
      "Best model saved, epoch 13\n",
      "Start training in split train, Epoch: 14\n",
      "Epoch 14: [10 / 21] loss: 0.585\n",
      "Epoch 14: [20 / 21] loss: 0.561\n",
      "Evaluation accuracy in `test` split: 0.571\n",
      "Best model saved, epoch 14\n",
      "Start training in split train, Epoch: 15\n",
      "Epoch 15: [10 / 21] loss: 0.562\n",
      "Epoch 15: [20 / 21] loss: 0.539\n",
      "Evaluation accuracy in `test` split: 0.579\n",
      "Best model saved, epoch 15\n",
      "Start training in split train, Epoch: 16\n",
      "Epoch 16: [10 / 21] loss: 0.531\n",
      "Epoch 16: [20 / 21] loss: 0.550\n",
      "Evaluation accuracy in `test` split: 0.586\n",
      "Best model saved, epoch 16\n",
      "Start training in split train, Epoch: 17\n",
      "Epoch 17: [10 / 21] loss: 0.564\n",
      "Epoch 17: [20 / 21] loss: 0.564\n",
      "Evaluation accuracy in `test` split: 0.521\n",
      "Start training in split train, Epoch: 18\n",
      "Epoch 18: [10 / 21] loss: 0.546\n",
      "Epoch 18: [20 / 21] loss: 0.506\n",
      "Evaluation accuracy in `test` split: 0.621\n",
      "Best model saved, epoch 18\n",
      "Start training in split train, Epoch: 19\n",
      "Epoch 19: [10 / 21] loss: 0.518\n",
      "Epoch 19: [20 / 21] loss: 0.503\n",
      "Evaluation accuracy in `test` split: 0.607\n",
      "Start training in split train, Epoch: 20\n",
      "Epoch 20: [10 / 21] loss: 0.500\n",
      "Epoch 20: [20 / 21] loss: 0.520\n",
      "Evaluation accuracy in `test` split: 0.679\n",
      "Best model saved, epoch 20\n",
      "Start training in split train, Epoch: 21\n",
      "Epoch 21: [10 / 21] loss: 0.495\n",
      "Epoch 21: [20 / 21] loss: 0.517\n",
      "Evaluation accuracy in `test` split: 0.664\n",
      "Start training in split train, Epoch: 22\n",
      "Epoch 22: [10 / 21] loss: 0.472\n",
      "Epoch 22: [20 / 21] loss: 0.510\n",
      "Evaluation accuracy in `test` split: 0.657\n",
      "Start training in split train, Epoch: 23\n",
      "Epoch 23: [10 / 21] loss: 0.481\n",
      "Epoch 23: [20 / 21] loss: 0.478\n",
      "Evaluation accuracy in `test` split: 0.707\n",
      "Best model saved, epoch 23\n",
      "Start training in split train, Epoch: 24\n",
      "Epoch 24: [10 / 21] loss: 0.465\n",
      "Epoch 24: [20 / 21] loss: 0.484\n",
      "Evaluation accuracy in `test` split: 0.679\n",
      "Start training in split train, Epoch: 25\n",
      "Epoch 25: [10 / 21] loss: 0.468\n",
      "Epoch 25: [20 / 21] loss: 0.472\n",
      "Evaluation accuracy in `test` split: 0.700\n",
      "Start training in split train, Epoch: 26\n",
      "Epoch 26: [10 / 21] loss: 0.462\n",
      "Epoch 26: [20 / 21] loss: 0.457\n",
      "Evaluation accuracy in `test` split: 0.707\n",
      "Best model saved, epoch 26\n",
      "Start training in split train, Epoch: 27\n",
      "Epoch 27: [10 / 21] loss: 0.430\n",
      "Epoch 27: [20 / 21] loss: 0.439\n",
      "Evaluation accuracy in `test` split: 0.721\n",
      "Best model saved, epoch 27\n",
      "Start training in split train, Epoch: 28\n",
      "Epoch 28: [10 / 21] loss: 0.454\n",
      "Epoch 28: [20 / 21] loss: 0.419\n",
      "Evaluation accuracy in `test` split: 0.714\n",
      "Start training in split train, Epoch: 29\n",
      "Epoch 29: [10 / 21] loss: 0.416\n",
      "Epoch 29: [20 / 21] loss: 0.458\n",
      "Evaluation accuracy in `test` split: 0.714\n",
      "Start training in split train, Epoch: 30\n",
      "Epoch 30: [10 / 21] loss: 0.409\n",
      "Epoch 30: [20 / 21] loss: 0.407\n",
      "Evaluation accuracy in `test` split: 0.771\n",
      "Best model saved, epoch 30\n",
      "Start training in split train, Epoch: 31\n",
      "Epoch 31: [10 / 21] loss: 0.421\n",
      "Epoch 31: [20 / 21] loss: 0.396\n",
      "Evaluation accuracy in `test` split: 0.764\n",
      "Start training in split train, Epoch: 32\n",
      "Epoch 32: [10 / 21] loss: 0.397\n",
      "Epoch 32: [20 / 21] loss: 0.432\n",
      "Evaluation accuracy in `test` split: 0.779\n",
      "Best model saved, epoch 32\n",
      "Start training in split train, Epoch: 33\n",
      "Epoch 33: [10 / 21] loss: 0.430\n",
      "Epoch 33: [20 / 21] loss: 0.405\n",
      "Evaluation accuracy in `test` split: 0.729\n",
      "Start training in split train, Epoch: 34\n",
      "Epoch 34: [10 / 21] loss: 0.425\n",
      "Epoch 34: [20 / 21] loss: 0.408\n",
      "Evaluation accuracy in `test` split: 0.757\n",
      "Start training in split train, Epoch: 35\n",
      "Epoch 35: [10 / 21] loss: 0.393\n",
      "Epoch 35: [20 / 21] loss: 0.393\n",
      "Evaluation accuracy in `test` split: 0.800\n",
      "Best model saved, epoch 35\n",
      "Start training in split train, Epoch: 36\n",
      "Epoch 36: [10 / 21] loss: 0.394\n",
      "Epoch 36: [20 / 21] loss: 0.408\n",
      "Evaluation accuracy in `test` split: 0.779\n",
      "Start training in split train, Epoch: 37\n",
      "Epoch 37: [10 / 21] loss: 0.403\n",
      "Epoch 37: [20 / 21] loss: 0.395\n",
      "Evaluation accuracy in `test` split: 0.800\n",
      "Best model saved, epoch 37\n",
      "Start training in split train, Epoch: 38\n",
      "Epoch 38: [10 / 21] loss: 0.386\n",
      "Epoch 38: [20 / 21] loss: 0.391\n",
      "Evaluation accuracy in `test` split: 0.814\n",
      "Best model saved, epoch 38\n",
      "Start training in split train, Epoch: 39\n",
      "Epoch 39: [10 / 21] loss: 0.386\n",
      "Epoch 39: [20 / 21] loss: 0.361\n",
      "Evaluation accuracy in `test` split: 0.807\n",
      "Start training in split train, Epoch: 40\n",
      "Epoch 40: [10 / 21] loss: 0.376\n",
      "Epoch 40: [20 / 21] loss: 0.370\n",
      "Evaluation accuracy in `test` split: 0.821\n",
      "Best model saved, epoch 40\n",
      "Start training in split train, Epoch: 41\n",
      "Epoch 41: [10 / 21] loss: 0.358\n",
      "Epoch 41: [20 / 21] loss: 0.379\n",
      "Evaluation accuracy in `test` split: 0.786\n",
      "Start training in split train, Epoch: 42\n",
      "Epoch 42: [10 / 21] loss: 0.376\n",
      "Epoch 42: [20 / 21] loss: 0.376\n",
      "Evaluation accuracy in `test` split: 0.850\n",
      "Best model saved, epoch 42\n",
      "Start training in split train, Epoch: 43\n",
      "Epoch 43: [10 / 21] loss: 0.376\n",
      "Epoch 43: [20 / 21] loss: 0.378\n",
      "Evaluation accuracy in `test` split: 0.829\n",
      "Start training in split train, Epoch: 44\n",
      "Epoch 44: [10 / 21] loss: 0.388\n",
      "Epoch 44: [20 / 21] loss: 0.386\n",
      "Evaluation accuracy in `test` split: 0.821\n",
      "Start training in split train, Epoch: 45\n",
      "Epoch 45: [10 / 21] loss: 0.381\n",
      "Epoch 45: [20 / 21] loss: 0.359\n",
      "Evaluation accuracy in `test` split: 0.850\n",
      "Best model saved, epoch 45\n",
      "Start training in split train, Epoch: 46\n",
      "Epoch 46: [10 / 21] loss: 0.369\n",
      "Epoch 46: [20 / 21] loss: 0.392\n",
      "Evaluation accuracy in `test` split: 0.836\n",
      "Start training in split train, Epoch: 47\n",
      "Epoch 47: [10 / 21] loss: 0.357\n",
      "Epoch 47: [20 / 21] loss: 0.373\n",
      "Evaluation accuracy in `test` split: 0.843\n",
      "Start training in split train, Epoch: 48\n",
      "Epoch 48: [10 / 21] loss: 0.352\n",
      "Epoch 48: [20 / 21] loss: 0.367\n",
      "Evaluation accuracy in `test` split: 0.850\n",
      "Best model saved, epoch 48\n",
      "Start training in split train, Epoch: 49\n",
      "Epoch 49: [10 / 21] loss: 0.358\n",
      "Epoch 49: [20 / 21] loss: 0.352\n",
      "Evaluation accuracy in `test` split: 0.871\n",
      "Best model saved, epoch 49\n",
      "Start training in split train, Epoch: 50\n",
      "Epoch 50: [10 / 21] loss: 0.342\n",
      "Epoch 50: [20 / 21] loss: 0.357\n",
      "Evaluation accuracy in `test` split: 0.857\n",
      "Start training in split train, Epoch: 51\n",
      "Epoch 51: [10 / 21] loss: 0.362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: [20 / 21] loss: 0.360\n",
      "Evaluation accuracy in `test` split: 0.821\n",
      "Start training in split train, Epoch: 52\n",
      "Epoch 52: [10 / 21] loss: 0.360\n",
      "Epoch 52: [20 / 21] loss: 0.345\n",
      "Evaluation accuracy in `test` split: 0.857\n",
      "Start training in split train, Epoch: 53\n",
      "Epoch 53: [10 / 21] loss: 0.361\n",
      "Epoch 53: [20 / 21] loss: 0.337\n",
      "Evaluation accuracy in `test` split: 0.850\n",
      "Start training in split train, Epoch: 54\n",
      "Epoch 54: [10 / 21] loss: 0.346\n",
      "Epoch 54: [20 / 21] loss: 0.354\n",
      "Evaluation accuracy in `test` split: 0.836\n",
      "Start training in split train, Epoch: 55\n",
      "Epoch 55: [10 / 21] loss: 0.359\n",
      "Epoch 55: [20 / 21] loss: 0.365\n",
      "Evaluation accuracy in `test` split: 0.857\n",
      "Start training in split train, Epoch: 56\n",
      "Epoch 56: [10 / 21] loss: 0.365\n",
      "Epoch 56: [20 / 21] loss: 0.358\n",
      "Evaluation accuracy in `test` split: 0.857\n",
      "Start training in split train, Epoch: 57\n",
      "Epoch 57: [10 / 21] loss: 0.349\n",
      "Epoch 57: [20 / 21] loss: 0.346\n",
      "Evaluation accuracy in `test` split: 0.886\n",
      "Best model saved, epoch 57\n",
      "Start training in split train, Epoch: 58\n",
      "Epoch 58: [10 / 21] loss: 0.350\n",
      "Epoch 58: [20 / 21] loss: 0.348\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Best model saved, epoch 58\n",
      "Start training in split train, Epoch: 59\n",
      "Epoch 59: [10 / 21] loss: 0.361\n",
      "Epoch 59: [20 / 21] loss: 0.350\n",
      "Evaluation accuracy in `test` split: 0.857\n",
      "Start training in split train, Epoch: 60\n",
      "Epoch 60: [10 / 21] loss: 0.355\n",
      "Epoch 60: [20 / 21] loss: 0.340\n",
      "Evaluation accuracy in `test` split: 0.857\n",
      "Start training in split train, Epoch: 61\n",
      "Epoch 61: [10 / 21] loss: 0.335\n",
      "Epoch 61: [20 / 21] loss: 0.346\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Best model saved, epoch 61\n",
      "Start training in split train, Epoch: 62\n",
      "Epoch 62: [10 / 21] loss: 0.369\n",
      "Epoch 62: [20 / 21] loss: 0.369\n",
      "Evaluation accuracy in `test` split: 0.843\n",
      "Start training in split train, Epoch: 63\n",
      "Epoch 63: [10 / 21] loss: 0.327\n",
      "Epoch 63: [20 / 21] loss: 0.357\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Start training in split train, Epoch: 64\n",
      "Epoch 64: [10 / 21] loss: 0.358\n",
      "Epoch 64: [20 / 21] loss: 0.345\n",
      "Evaluation accuracy in `test` split: 0.800\n",
      "Start training in split train, Epoch: 65\n",
      "Epoch 65: [10 / 21] loss: 0.341\n",
      "Epoch 65: [20 / 21] loss: 0.342\n",
      "Evaluation accuracy in `test` split: 0.843\n",
      "Start training in split train, Epoch: 66\n",
      "Epoch 66: [10 / 21] loss: 0.348\n",
      "Epoch 66: [20 / 21] loss: 0.336\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Start training in split train, Epoch: 67\n",
      "Epoch 67: [10 / 21] loss: 0.334\n",
      "Epoch 67: [20 / 21] loss: 0.334\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Start training in split train, Epoch: 68\n",
      "Epoch 68: [10 / 21] loss: 0.328\n",
      "Epoch 68: [20 / 21] loss: 0.329\n",
      "Evaluation accuracy in `test` split: 0.886\n",
      "Start training in split train, Epoch: 69\n",
      "Epoch 69: [10 / 21] loss: 0.330\n",
      "Epoch 69: [20 / 21] loss: 0.328\n",
      "Evaluation accuracy in `test` split: 0.886\n",
      "Start training in split train, Epoch: 70\n",
      "Epoch 70: [10 / 21] loss: 0.334\n",
      "Epoch 70: [20 / 21] loss: 0.321\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Start training in split train, Epoch: 71\n",
      "Epoch 71: [10 / 21] loss: 0.334\n",
      "Epoch 71: [20 / 21] loss: 0.326\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Best model saved, epoch 71\n",
      "Start training in split train, Epoch: 72\n",
      "Epoch 72: [10 / 21] loss: 0.311\n",
      "Epoch 72: [20 / 21] loss: 0.317\n",
      "Evaluation accuracy in `test` split: 0.907\n",
      "Best model saved, epoch 72\n",
      "Start training in split train, Epoch: 73\n",
      "Epoch 73: [10 / 21] loss: 0.329\n",
      "Epoch 73: [20 / 21] loss: 0.346\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Start training in split train, Epoch: 74\n",
      "Epoch 74: [10 / 21] loss: 0.335\n",
      "Epoch 74: [20 / 21] loss: 0.323\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 75\n",
      "Epoch 75: [10 / 21] loss: 0.321\n",
      "Epoch 75: [20 / 21] loss: 0.319\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Start training in split train, Epoch: 76\n",
      "Epoch 76: [10 / 21] loss: 0.320\n",
      "Epoch 76: [20 / 21] loss: 0.342\n",
      "Evaluation accuracy in `test` split: 0.871\n",
      "Start training in split train, Epoch: 77\n",
      "Epoch 77: [10 / 21] loss: 0.315\n",
      "Epoch 77: [20 / 21] loss: 0.315\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 78\n",
      "Epoch 78: [10 / 21] loss: 0.315\n",
      "Epoch 78: [20 / 21] loss: 0.314\n",
      "Evaluation accuracy in `test` split: 0.871\n",
      "Start training in split train, Epoch: 79\n",
      "Epoch 79: [10 / 21] loss: 0.319\n",
      "Epoch 79: [20 / 21] loss: 0.318\n",
      "Evaluation accuracy in `test` split: 0.886\n",
      "Start training in split train, Epoch: 80\n",
      "Epoch 80: [10 / 21] loss: 0.320\n",
      "Epoch 80: [20 / 21] loss: 0.303\n",
      "Evaluation accuracy in `test` split: 0.871\n",
      "Start training in split train, Epoch: 81\n",
      "Epoch 81: [10 / 21] loss: 0.313\n",
      "Epoch 81: [20 / 21] loss: 0.311\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Start training in split train, Epoch: 82\n",
      "Epoch 82: [10 / 21] loss: 0.314\n",
      "Epoch 82: [20 / 21] loss: 0.312\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Start training in split train, Epoch: 83\n",
      "Epoch 83: [10 / 21] loss: 0.321\n",
      "Epoch 83: [20 / 21] loss: 0.302\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 84\n",
      "Epoch 84: [10 / 21] loss: 0.319\n",
      "Epoch 84: [20 / 21] loss: 0.316\n",
      "Evaluation accuracy in `test` split: 0.886\n",
      "Start training in split train, Epoch: 85\n",
      "Epoch 85: [10 / 21] loss: 0.318\n",
      "Epoch 85: [20 / 21] loss: 0.315\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 86\n",
      "Epoch 86: [10 / 21] loss: 0.310\n",
      "Epoch 86: [20 / 21] loss: 0.310\n",
      "Evaluation accuracy in `test` split: 0.921\n",
      "Best model saved, epoch 86\n",
      "Start training in split train, Epoch: 87\n",
      "Epoch 87: [10 / 21] loss: 0.321\n",
      "Epoch 87: [20 / 21] loss: 0.316\n",
      "Evaluation accuracy in `test` split: 0.907\n",
      "Start training in split train, Epoch: 88\n",
      "Epoch 88: [10 / 21] loss: 0.323\n",
      "Epoch 88: [20 / 21] loss: 0.317\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 89\n",
      "Epoch 89: [10 / 21] loss: 0.314\n",
      "Epoch 89: [20 / 21] loss: 0.321\n",
      "Evaluation accuracy in `test` split: 0.886\n",
      "Start training in split train, Epoch: 90\n",
      "Epoch 90: [10 / 21] loss: 0.302\n",
      "Epoch 90: [20 / 21] loss: 0.316\n",
      "Evaluation accuracy in `test` split: 0.864\n",
      "Start training in split train, Epoch: 91\n",
      "Epoch 91: [10 / 21] loss: 0.304\n",
      "Epoch 91: [20 / 21] loss: 0.310\n",
      "Evaluation accuracy in `test` split: 0.886\n",
      "Start training in split train, Epoch: 92\n",
      "Epoch 92: [10 / 21] loss: 0.307\n",
      "Epoch 92: [20 / 21] loss: 0.312\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Start training in split train, Epoch: 93\n",
      "Epoch 93: [10 / 21] loss: 0.286\n",
      "Epoch 93: [20 / 21] loss: 0.319\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 94\n",
      "Epoch 94: [10 / 21] loss: 0.314\n",
      "Epoch 94: [20 / 21] loss: 0.312\n",
      "Evaluation accuracy in `test` split: 0.900\n",
      "Start training in split train, Epoch: 95\n",
      "Epoch 95: [10 / 21] loss: 0.316\n",
      "Epoch 95: [20 / 21] loss: 0.308\n",
      "Evaluation accuracy in `test` split: 0.879\n",
      "Start training in split train, Epoch: 96\n",
      "Epoch 96: [10 / 21] loss: 0.302\n",
      "Epoch 96: [20 / 21] loss: 0.312\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Start training in split train, Epoch: 97\n",
      "Epoch 97: [10 / 21] loss: 0.304\n",
      "Epoch 97: [20 / 21] loss: 0.292\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Start training in split train, Epoch: 98\n",
      "Epoch 98: [10 / 21] loss: 0.298\n",
      "Epoch 98: [20 / 21] loss: 0.295\n",
      "Evaluation accuracy in `test` split: 0.886\n",
      "Start training in split train, Epoch: 99\n",
      "Epoch 99: [10 / 21] loss: 0.308\n",
      "Epoch 99: [20 / 21] loss: 0.308\n",
      "Evaluation accuracy in `test` split: 0.871\n",
      "Start training in split train, Epoch: 100\n",
      "Epoch 100: [10 / 21] loss: 0.311\n",
      "Epoch 100: [20 / 21] loss: 0.292\n",
      "Evaluation accuracy in `test` split: 0.871\n",
      "Start training in split train, Epoch: 101\n",
      "Epoch 101: [10 / 21] loss: 0.303\n",
      "Epoch 101: [20 / 21] loss: 0.284\n",
      "Evaluation accuracy in `test` split: 0.886\n",
      "Start training in split train, Epoch: 102\n",
      "Epoch 102: [10 / 21] loss: 0.307\n",
      "Epoch 102: [20 / 21] loss: 0.294\n",
      "Evaluation accuracy in `test` split: 0.871\n",
      "Start training in split train, Epoch: 103\n",
      "Epoch 103: [10 / 21] loss: 0.301\n",
      "Epoch 103: [20 / 21] loss: 0.312\n",
      "Evaluation accuracy in `test` split: 0.893\n",
      "Start training in split train, Epoch: 104\n",
      "Epoch 104: [10 / 21] loss: 0.301\n",
      "Epoch 104: [20 / 21] loss: 0.306\n",
      "Evaluation accuracy in `test` split: 0.879\n",
      "Start training in split train, Epoch: 105\n",
      "Epoch 105: [10 / 21] loss: 0.322\n",
      "Epoch 105: [20 / 21] loss: 0.416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy in `test` split: 0.886\n",
      "Start training in split train, Epoch: 106\n",
      "Epoch 106: [10 / 21] loss: 0.376\n",
      "Epoch 106: [20 / 21] loss: 0.361\n",
      "Evaluation accuracy in `test` split: 0.871\n",
      "Start training in split train, Epoch: 107\n",
      "Epoch 107: [10 / 21] loss: 0.328\n",
      "Epoch 107: [20 / 21] loss: 0.347\n"
     ]
    }
   ],
   "source": [
    "# run the model\n",
    "runner = Jobs(opt)\n",
    "max_score = runner.train()\n",
    "print(\"Train finish, best val score: {:.3f}\".format(max_score))\n",
    "test_score = runner.translate()\n",
    "print('Final test accuracy: {}'.format(test_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
