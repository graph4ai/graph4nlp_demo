{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph4NLP Demo: Math Word Problem\n",
    "---\n",
    "In this demo, we will have a closer look at how to apply **Graph2Tree model to the task of math word problem automatically solving**.\n",
    "Math word problem solving aims to infer reasonable equations from given natural language problem descriptions. It is important for exploring automatic solutions to mathematical problems and improving the reasoning ability of neural networks. \n",
    "In this demo, we use the Graph4NLP library to build a GNN-based math word problem (MWP) solving model. \n",
    "\n",
    "The **Graph2Tree** model consists of:\n",
    "- graph construction module (e.g., node embedding based dynamic graph)\n",
    "- graph embedding module (e.g., undirected GraphSage)\n",
    "- predictoin module (e.g., tree decoder with attention and copy mechanisms)\n",
    "\n",
    "As shown in the picture below, we firstly construct graph input from problem description by syntactic parsing (CoreNLP) and then represent the output equation with a hierarchical structure (Node ``N`` stands for non-terminal node).\n",
    "<p align=\"center\">\n",
    "<img src=\"../../docs/source/tutorial/imgs/g2t.png\" width=\"600\" class=\"center\" alt=\"graph2tree_mwp\"/>\n",
    "    <br/>\n",
    "</p>\n",
    "We will use the built-in Graph2Tree model APIs to build the model, and evaluate it on the Mawps dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "---\n",
    "1. Create virtual environment\n",
    "```\n",
    "conda create --name graph4nlp python=3.7\n",
    "conda activate graph4nlp\n",
    "```\n",
    "\n",
    "2. Install [graph4nlp](https://github.com/graph4ai/graph4nlp) library\n",
    "- Clone the github repo\n",
    "```\n",
    "git clone -b stable https://github.com/graph4ai/graph4nlp.git\n",
    "cd graph4nlp\n",
    "```\n",
    "- Then run `./configure` (or `./configure.bat` if you are using Windows 10) to config your installation. The configuration program will ask you to specify your CUDA version. If you do not have a GPU, please choose 'cpu'.\n",
    "```\n",
    "./configure\n",
    "```\n",
    "- Finally, install the package\n",
    "```\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "3. Set up StanfordCoreNLP (for static graph construction only, unnecessary for this demo because preprocessed data is provided)\n",
    "- Download [StanfordCoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n",
    "- Go to the root folder and start the server\n",
    "```\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T13:51:01.626460Z",
     "start_time": "2021-07-22T13:51:01.615750Z"
    }
   },
   "source": [
    "## Load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T12:50:10.012209Z",
     "start_time": "2021-07-25T12:50:09.979663Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 20,\n",
      " 'beam_size': 4,\n",
      " 'checkpoint_save_path': './checkpoint_save',\n",
      " 'dataset_yaml': './config.yaml',\n",
      " 'decoder_args': {'rnn_decoder_private': {'max_decoder_step': 50,\n",
      "                                          'max_tree_depth': 50,\n",
      "                                          'use_sibling': False},\n",
      "                  'rnn_decoder_share': {'attention_type': 'uniform',\n",
      "                                        'dropout': 0.1,\n",
      "                                        'fuse_strategy': 'concatenate',\n",
      "                                        'graph_pooling_strategy': None,\n",
      "                                        'hidden_size': 300,\n",
      "                                        'input_size': 300,\n",
      "                                        'rnn_emb_input_size': 300,\n",
      "                                        'rnn_type': 'lstm',\n",
      "                                        'teacher_forcing_rate': 1.0,\n",
      "                                        'use_copy': True,\n",
      "                                        'use_coverage': False}},\n",
      " 'decoder_name': 'stdtree',\n",
      " 'gpuid': 1,\n",
      " 'grad_clip': 5,\n",
      " 'graph_construction_args': {'graph_construction_private': {'as_node': False,\n",
      "                                                            'edge_strategy': 'homogeneous',\n",
      "                                                            'merge_strategy': 'tailhead',\n",
      "                                                            'sequential_link': True},\n",
      "                             'graph_construction_share': {'graph_type': 'dependency',\n",
      "                                                          'port': 9000,\n",
      "                                                          'root_dir': 'examples/pytorch/semantic_parsing/graph2tree/jobs/jobs_data',\n",
      "                                                          'share_vocab': True,\n",
      "                                                          'thread_number': 4,\n",
      "                                                          'timeout': 15000,\n",
      "                                                          'topology_subdir': 'DependencyGraph'},\n",
      "                             'node_embedding': {'connectivity_ratio': 0.05,\n",
      "                                                'embedding_style': {'bert_lower_case': None,\n",
      "                                                                    'bert_model_name': None,\n",
      "                                                                    'emb_strategy': 'w2v_bilstm',\n",
      "                                                                    'num_rnn_layers': 1,\n",
      "                                                                    'single_token_item': True},\n",
      "                                                'epsilon_neigh': 0.5,\n",
      "                                                'fix_bert_emb': False,\n",
      "                                                'fix_word_emb': False,\n",
      "                                                'hidden_size': 300,\n",
      "                                                'input_size': 300,\n",
      "                                                'num_heads': 1,\n",
      "                                                'rnn_dropout': 0.1,\n",
      "                                                'sim_metric_type': 'weighted_cosine',\n",
      "                                                'smoothness_ratio': 0.1,\n",
      "                                                'sparsity_ratio': 0.1,\n",
      "                                                'top_k_neigh': None,\n",
      "                                                'word_dropout': 0.1}},\n",
      " 'graph_construction_name': 'dependency',\n",
      " 'graph_embedding_args': {'graph_embedding_private': {'activation': 'relu',\n",
      "                                                      'aggregator_type': 'lstm',\n",
      "                                                      'bias': True,\n",
      "                                                      'norm': None,\n",
      "                                                      'use_edge_weight': False},\n",
      "                          'graph_embedding_share': {'attn_drop': 0.0,\n",
      "                                                    'direction_option': 'undirected',\n",
      "                                                    'feat_drop': 0.0,\n",
      "                                                    'hidden_size': 300,\n",
      "                                                    'input_size': 300,\n",
      "                                                    'num_layers': 1,\n",
      "                                                    'output_size': 300}},\n",
      " 'graph_embedding_name': 'graphsage',\n",
      " 'graph_type': 'static',\n",
      " 'init_weight': 0.08,\n",
      " 'learning_rate': 0.001,\n",
      " 'max_epochs': 200,\n",
      " 'min_freq': 1,\n",
      " 'pretrained_word_emb_cache_dir': '.vector_cache',\n",
      " 'pretrained_word_emb_name': None,\n",
      " 'pretrained_word_emb_url': None,\n",
      " 'seed': 1237,\n",
      " 'share_vocab': True,\n",
      " 'weight_decay': 0}\n"
     ]
    }
   ],
   "source": [
    "from graph4nlp.pytorch.modules.config import get_basic_args\n",
    "from graph4nlp.pytorch.modules.utils.config_utils import update_values, get_yaml_config\n",
    "\n",
    "def get_args():\n",
    "    config = {'dataset_yaml': \"./config.yaml\",\n",
    "              'learning_rate': 1e-3,\n",
    "              'gpuid': 1,\n",
    "              'seed': 1237, \n",
    "              'init_weight': 0.08,\n",
    "              'graph_type': 'static',\n",
    "              'weight_decay': 0, \n",
    "              'max_epochs': 200, \n",
    "              'min_freq': 1,\n",
    "              'grad_clip': 5,\n",
    "              'batch_size': 20,\n",
    "              'share_vocab': True,\n",
    "              'pretrained_word_emb_name': None,\n",
    "              'pretrained_word_emb_url': None,\n",
    "              'pretrained_word_emb_cache_dir': \".vector_cache\",\n",
    "              'checkpoint_save_path': \"./checkpoint_save\",\n",
    "              'beam_size': 4\n",
    "              }\n",
    "    our_args = get_yaml_config(config['dataset_yaml'])\n",
    "    template = get_basic_args(graph_construction_name=our_args[\"graph_construction_name\"],\n",
    "                              graph_embedding_name=our_args[\"graph_embedding_name\"],\n",
    "                              decoder_name=our_args[\"decoder_name\"])\n",
    "    update_values(to_args=template, from_args_list=[our_args, config])\n",
    "    return template\n",
    "\n",
    "# show our config\n",
    "cfg_g2t = get_args()\n",
    "from pprint import pprint\n",
    "pprint(cfg_g2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T12:52:47.276757Z",
     "start_time": "2021-07-25T12:52:47.261047Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from graph4nlp.pytorch.data.data import to_batch\n",
    "from graph4nlp.pytorch.datasets.mawps import MawpsDatasetForTree\n",
    "from graph4nlp.pytorch.modules.graph_construction import DependencyBasedGraphConstruction\n",
    "from graph4nlp.pytorch.modules.graph_embedding import *\n",
    "from graph4nlp.pytorch.models.graph2tree import Graph2Tree\n",
    "from graph4nlp.pytorch.modules.utils.tree_utils import Tree, VocabForAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T13:51:40.495148Z",
     "start_time": "2021-07-22T13:51:40.465080Z"
    }
   },
   "outputs": [],
   "source": [
    "class Mawps:\n",
    "    def __init__(self, opt=None):\n",
    "        super(Mawps, self).__init__()\n",
    "        self.opt = opt\n",
    "\n",
    "        seed = self.opt[\"seed\"]\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        if self.opt[\"gpuid\"] == -1:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda:{}\".format(self.opt[\"gpuid\"]))\n",
    "\n",
    "        self.use_copy = self.opt[\"decoder_args\"][\"rnn_decoder_share\"][\"use_copy\"]\n",
    "        self.use_share_vocab = self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"share_vocab\"]\n",
    "        self.data_dir = self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"root_dir\"]\n",
    "\n",
    "        self._build_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "\n",
    "    def _build_dataloader(self):\n",
    "        graph_type = self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"graph_type\"]\n",
    "        enc_emb_size = self.opt[\"graph_construction_args\"][\"node_embedding\"][\"input_size\"]\n",
    "        tgt_emb_size = self.opt[\"decoder_args\"][\"rnn_decoder_share\"][\"input_size\"]\n",
    "        topology_subdir = self.opt[\"graph_construction_args\"][\"graph_construction_share\"][\"topology_subdir\"]\n",
    "        \n",
    "        para_dic =  {'root_dir': self.data_dir,\n",
    "                    'word_emb_size': enc_emb_size,\n",
    "                    'topology_builder': DependencyBasedGraphConstruction,\n",
    "                    'topology_subdir': topology_subdir, \n",
    "                    'edge_strategy': self.opt[\"graph_construction_args\"][\"graph_construction_private\"][\"edge_strategy\"],\n",
    "                    'graph_type': 'static',\n",
    "                    'dynamic_graph_type': graph_type, \n",
    "                    'share_vocab': self.use_share_vocab, \n",
    "                    'enc_emb_size': enc_emb_size,\n",
    "                    'dec_emb_size': tgt_emb_size,\n",
    "                    'dynamic_init_topology_builder': None,\n",
    "                    'min_word_vocab_freq': self.opt[\"min_freq\"],\n",
    "                    'pretrained_word_emb_name': self.opt[\"pretrained_word_emb_name\"],\n",
    "                    'pretrained_word_emb_url': self.opt[\"pretrained_word_emb_url\"], \n",
    "                    'pretrained_word_emb_cache_dir': self.opt[\"pretrained_word_emb_cache_dir\"]\n",
    "                    }\n",
    "\n",
    "        dataset = MawpsDatasetForTree(**para_dic)\n",
    "\n",
    "        self.train_data_loader = DataLoader(dataset.train, batch_size=self.opt[\"batch_size\"], shuffle=True,\n",
    "                                            num_workers=0,\n",
    "                                            collate_fn=dataset.collate_fn)\n",
    "        self.test_data_loader = DataLoader(dataset.test, batch_size=1, shuffle=False, num_workers=0,\n",
    "                                           collate_fn=dataset.collate_fn)\n",
    "        self.valid_data_loader = DataLoader(dataset.val, batch_size=1, shuffle=False, num_workers=0,\n",
    "                                          collate_fn=dataset.collate_fn)\n",
    "        self.vocab_model = dataset.vocab_model\n",
    "        self.src_vocab = self.vocab_model.in_word_vocab\n",
    "        self.tgt_vocab = self.vocab_model.out_word_vocab\n",
    "        self.share_vocab = self.vocab_model.share_vocab if self.use_share_vocab else None\n",
    "\n",
    "    def _build_model(self):\n",
    "        '''For encoder-decoder'''\n",
    "        self.model = Graph2Tree.from_args(self.opt, vocab_model=self.vocab_model)\n",
    "        self.model.init(self.opt[\"init_weight\"])\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        optim_state = {\"learningRate\": self.opt[\"learning_rate\"], \"weight_decay\": self.opt[\"weight_decay\"]}\n",
    "        parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = optim.Adam(parameters, lr=optim_state['learningRate'], weight_decay=optim_state['weight_decay'])\n",
    "\n",
    "    def prepare_ext_vocab(self, batch_graph, src_vocab):\n",
    "        oov_dict = copy.deepcopy(src_vocab)\n",
    "        token_matrix = []\n",
    "        for n in batch_graph.node_attributes:\n",
    "            node_token = n['token']\n",
    "            if (n.get('type') == None or n.get('type') == 0) and oov_dict.get_symbol_idx(node_token) == oov_dict.get_symbol_idx(oov_dict.unk_token):\n",
    "                oov_dict.add_symbol(node_token)\n",
    "            token_matrix.append(oov_dict.get_symbol_idx(node_token))\n",
    "        batch_graph.node_features['token_id_oov'] = torch.tensor(token_matrix, dtype=torch.long).to(self.device)\n",
    "        return oov_dict\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        loss_to_print = 0\n",
    "        num_batch = len(self.train_data_loader)\n",
    "        for step, data in tqdm(enumerate(self.train_data_loader), desc=f'Epoch {epoch:02d}', total=len(self.train_data_loader)):\n",
    "            batch_graph, batch_tree_list, batch_original_tree_list = data['graph_data'], data['dec_tree_batch'], data['original_dec_tree_batch']\n",
    "            batch_graph = batch_graph.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            oov_dict = self.prepare_ext_vocab(\n",
    "                batch_graph, self.src_vocab) if self.use_copy else None\n",
    "\n",
    "            if self.use_copy:\n",
    "                batch_tree_list_refined = []\n",
    "                for item in batch_original_tree_list:\n",
    "                    tgt_list = oov_dict.get_symbol_idx_for_list(item.strip().split())\n",
    "                    tgt_tree = Tree.convert_to_tree(tgt_list, 0, len(tgt_list), oov_dict)\n",
    "                    batch_tree_list_refined.append(tgt_tree)\n",
    "            loss = self.model(batch_graph, batch_tree_list_refined if self.use_copy else batch_tree_list, oov_dict=oov_dict)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                self.model.parameters(), self.opt[\"grad_clip\"])\n",
    "            self.optimizer.step()\n",
    "            loss_to_print += loss\n",
    "        return loss_to_print/num_batch\n",
    "\n",
    "    def train(self):\n",
    "        best_acc = (-1, -1)\n",
    "        best_model = None\n",
    "\n",
    "        print(\"-------------\\nStarting training.\")\n",
    "        for epoch in range(1, self.opt[\"max_epochs\"]+1):\n",
    "            self.model.train()\n",
    "            loss_to_print = self.train_epoch(epoch)\n",
    "            print(\"epochs = {}, train_loss = {:.3f}\".format(epoch, loss_to_print))\n",
    "            if epoch > 2 and epoch % 1 == 0:\n",
    "                test_acc = self.eval(self.model, mode=\"test\")\n",
    "                val_acc = self.eval(self.model, mode=\"val\")\n",
    "                if val_acc > best_acc[1]:\n",
    "                    best_acc = (test_acc, val_acc)\n",
    "                    best_model = self.model\n",
    "        print(\"Best Acc: {:.3f}\\n\".format(best_acc[0]))\n",
    "        best_model.save_checkpoint(self.opt[\"checkpoint_save_path\"], \"best.pt\")\n",
    "        return best_acc\n",
    "\n",
    "    def eval(self, model, mode=\"val\"):\n",
    "        from evaluation import convert_to_string, compute_tree_accuracy\n",
    "        model.eval()\n",
    "        reference_list = []\n",
    "        candidate_list = []\n",
    "        data_loader = self.test_data_loader if mode == \"test\" else self.valid_data_loader\n",
    "        for data in tqdm(data_loader, desc=\"Eval: \"):\n",
    "            eval_input_graph, batch_tree_list, batch_original_tree_list = data['graph_data'], data['dec_tree_batch'], data['original_dec_tree_batch']\n",
    "            eval_input_graph = eval_input_graph.to(self.device)\n",
    "            oov_dict = self.prepare_ext_vocab(eval_input_graph, self.src_vocab)\n",
    "\n",
    "            if self.use_copy:\n",
    "                assert len(batch_original_tree_list) == 1\n",
    "                reference = oov_dict.get_symbol_idx_for_list(batch_original_tree_list[0].split())\n",
    "                eval_vocab = oov_dict\n",
    "            else:\n",
    "                assert len(batch_original_tree_list) == 1\n",
    "                reference = model.tgt_vocab.get_symbol_idx_for_list(batch_original_tree_list[0].split())\n",
    "                eval_vocab = self.tgt_vocab\n",
    "            \n",
    "            candidate = model.translate(eval_input_graph,\n",
    "                                        oov_dict=oov_dict,\n",
    "                                        use_beam_search=True,\n",
    "                                        beam_size=self.opt[\"beam_size\"])\n",
    "            \n",
    "            candidate = [int(c) for c in candidate]\n",
    "            num_left_paren = sum(\n",
    "                1 for c in candidate if eval_vocab.idx2symbol[int(c)] == \"(\")\n",
    "            num_right_paren = sum(\n",
    "                1 for c in candidate if eval_vocab.idx2symbol[int(c)] == \")\")\n",
    "            diff = num_left_paren - num_right_paren\n",
    "            if diff > 0:\n",
    "                for i in range(diff):\n",
    "                    candidate.append(\n",
    "                        self.test_data_loader.tgt_vocab.symbol2idx[\")\"])\n",
    "            elif diff < 0:\n",
    "                candidate = candidate[:diff]\n",
    "            ref_str = convert_to_string(\n",
    "                reference, eval_vocab)\n",
    "            cand_str = convert_to_string(\n",
    "                candidate, eval_vocab)\n",
    "\n",
    "            reference_list.append(reference)\n",
    "            candidate_list.append(candidate)\n",
    "        eval_acc = compute_tree_accuracy(\n",
    "            candidate_list, reference_list, eval_vocab)\n",
    "        print(\"{} accuracy = {:.3f}\\n\".format(mode, eval_acc))\n",
    "        return eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from random import randint\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "\n",
    "def convert_to_string(idx_list, form_manager):\n",
    "    w_list = []\n",
    "    for i in range(len(idx_list)):\n",
    "        w_list.append(form_manager.get_idx_symbol(int(idx_list[i])))\n",
    "    return \" \".join(w_list)\n",
    "\n",
    "def is_all_same(c1, c2, form_manager):\n",
    "    all_same = False\n",
    "    if len(c1) == len(c2):\n",
    "        all_same = True\n",
    "        for j in range(len(c1)):\n",
    "            if c1[j] != c2[j]:\n",
    "                all_same = False\n",
    "                break\n",
    "    if all_same == False:\n",
    "        if is_solution_same(c1, c2, form_manager):\n",
    "            return True\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def is_solution_same(i1, i2, form_manager):\n",
    "    c1 = \" \".join([form_manager.get_idx_symbol(x) for x in i1])\n",
    "    c2 = \" \".join([form_manager.get_idx_symbol(x) for x in i2])\n",
    "    if ('=' not in c1) or ('=' not in c2):\n",
    "        return False\n",
    "    elif (form_manager.unk_token in c1) or (form_manager.unk_token in c2):\n",
    "        return False\n",
    "    else:\n",
    "        try:\n",
    "            s1 = c1.split('=')\n",
    "            s2 = c2.split('=')\n",
    "            eq1 = []\n",
    "            eq2 = []\n",
    "            x = sympy.Symbol('x')\n",
    "            eq1.append(parse_expr(s1[0]))\n",
    "            eq1.append(parse_expr(s1[1]))\n",
    "            eq2.append(parse_expr(s2[0]))\n",
    "            eq2.append(parse_expr(s2[1]))\n",
    "            res1 = sympy.solve(sympy.Eq(eq1[0], eq1[1]), x)\n",
    "            res2 = sympy.solve(sympy.Eq(eq2[0], eq2[1]), x)\n",
    "\n",
    "            if not res1 or not res2:\n",
    "                return False\n",
    "            if res1[0] == res2[0]:\n",
    "                # print(\"Excution_true: \", c1, '\\t', c2)\n",
    "                pass\n",
    "            return res1[0] == res2[0]\n",
    "\n",
    "        except BaseException:\n",
    "            # print(\"Excution_error: \", c1, '\\t', c2)\n",
    "            pass\n",
    "            return False\n",
    "\n",
    "def compute_accuracy(candidate_list, reference_list, form_manager):\n",
    "    if len(candidate_list) != len(reference_list):\n",
    "        print(\"candidate list has length {}, reference list has length {}\\n\".format(\n",
    "            len(candidate_list), len(reference_list)))\n",
    "    len_min = min(len(candidate_list), len(reference_list))\n",
    "    c = 0\n",
    "    for i in range(len_min):\n",
    "        if is_all_same(candidate_list[i], reference_list[i], form_manager):\n",
    "            c = c+1\n",
    "        else:\n",
    "            pass\n",
    "    return c/float(len_min)\n",
    "\n",
    "\n",
    "def compute_tree_accuracy(candidate_list_, reference_list_, form_manager):\n",
    "    candidate_list = []\n",
    "    for i in range(len(candidate_list_)):\n",
    "        candidate_list.append(candidate_list_[i])\n",
    "    reference_list = []\n",
    "    for i in range(len(reference_list_)):\n",
    "        reference_list.append(reference_list_[i])\n",
    "    return compute_accuracy(candidate_list, reference_list, form_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from config import get_args\n",
    "    start = time.time()\n",
    "    runner = Mawps(opt=get_args())\n",
    "    best_acc = runner.train()\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"total time: {} minutes\\n\".format((end - start)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
