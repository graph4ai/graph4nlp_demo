{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Knowledge Graph Completion\n",
    "\n",
    "In this tutorial demo, we will use the Graph4NLP library to build a GNN-based knowledge graph completion model. The model consists of\n",
    "\n",
    "+ graph embedding module (e.g., GGNN)\n",
    "+ predictoin module (e.g., DistMult decoder)\n",
    "\n",
    "We will use the built-in Graph2Seq model APIs to build the model, and evaluate it on the Kinship dataset.\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "### Create virtual environment\n",
    "\n",
    "+ conda create --name g4l python=3.7\n",
    "+ conda activate g4l\n",
    "\n",
    "### Install graph4nlp library via pip\n",
    "\n",
    "#### Ensure that at least PyTorch (>=1.6.0) is installed:\n",
    "\n",
    "``` bash\n",
    "$ python -c \"import torch; print(torch.__version__)\"\n",
    ">>> 1.6.0\n",
    "```\n",
    "\n",
    "#### Find the CUDA version PyTorch was installed with (for GPU users):\n",
    "```bash\n",
    "$ python -c \"import torch; print(torch.version.cuda)\"\n",
    ">>> 10.2\n",
    "```\n",
    "\n",
    "#### Install the relevant dependencies:\n",
    "`torchtext` is needed since Graph4NLP relies on it to implement embeddings.\n",
    "Please pay attention to the PyTorch requirements before installing `torchtext` with the following script! For detailed version matching please refer [here](https://pypi.org/project/torchtext/).\n",
    "\n",
    "``` bash\n",
    "pip install torchtext # >=0.7.0\n",
    "```\n",
    "\n",
    "#### Install Graph4NLP\n",
    "\n",
    "```bash\n",
    "pip install graph4nlp${CUDA}\n",
    "```\n",
    "where `${CUDA}` should be replaced by the specific CUDA version (`none` (CPU version), `\"-cu92\"`, `\"-cu101\"`, `\"-cu102\"`, `\"-cu110\"`). The following table shows the concrete command lines. For CUDA 11.1 users, please refer to `Installation via source code`.\n",
    "\n",
    "| Platform  | Command                       |\n",
    "| --------- | ----------------------------- |\n",
    "| CPU       | `pip install graph4nlp`   |\n",
    "| CUDA 9.2  | `pip install graph4nlp-cu92`  |\n",
    "| CUDA 10.1 | `pip install graph4nlp-cu101` |\n",
    "| CUDA 10.2 | `pip install graph4nlp-cu102` |\n",
    "| CUDA 11.0 | `pip install graph4nlp-cu110` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Data preprocessing for KGC\n",
    "+ Download the default English model used by **spaCy**, which is installed in the previous step \n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "pip install h5py\n",
    "pip install future\n",
    "```\n",
    "+ Run the preprocessing script for WN18RR and Kinship: ```sh kg_completion/preprocess.sh```\n",
    "+ You can now run the model\n",
    "\n",
    "+ This notebook is tested on :\n",
    "\n",
    "```\n",
    "torch == 1.8.0\n",
    "torchtext == 0.9.0\n",
    "spacy == 3.0.7\n",
    "graph4nlp == 0.4.1 # macos, cpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from evaluation import ranking_and_hits\n",
    "from model import ConvE, Distmult, Complex, GGNNDistMult, GCNDistMult, GCNComplex\n",
    "\n",
    "from spodernet.preprocessing.pipeline import DatasetStreamer\n",
    "from spodernet.preprocessing.processors import JsonLoaderProcessors, Tokenizer, AddToVocab, SaveLengthsToState, StreamToHDF5, SaveMaxLengthsToState, CustomTokenizer\n",
    "from spodernet.preprocessing.processors import ConvertTokenToIdx, ApplyFunction, ToLower, DictKey2ListMapper, ApplyFunction, StreamToBatch\n",
    "from spodernet.utils.global_config import Config, Backends\n",
    "from spodernet.utils.logger import Logger, LogLevel\n",
    "from spodernet.preprocessing.batching import StreamBatcher\n",
    "from spodernet.preprocessing.pipeline import Pipeline\n",
    "from spodernet.preprocessing.processors import TargetIdx2MultiTarget\n",
    "from spodernet.hooks import LossHook, ETAHook\n",
    "from spodernet.utils.util import Timer\n",
    "from spodernet.preprocessing.processors import TargetIdx2MultiTarget\n",
    "import argparse\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preprocess knowledge graph using spodernet. '''\n",
    "def preprocess(dataset_name, delete_data=False):\n",
    "    full_path = 'data/{0}/e1rel_to_e2_full.json'.format(dataset_name)\n",
    "    train_path = 'data/{0}/e1rel_to_e2_train.json'.format(dataset_name)\n",
    "    dev_ranking_path = 'data/{0}/e1rel_to_e2_ranking_dev.json'.format(dataset_name)\n",
    "    test_ranking_path = 'data/{0}/e1rel_to_e2_ranking_test.json'.format(dataset_name)\n",
    "\n",
    "    keys2keys = {}\n",
    "    keys2keys['e1'] = 'e1' # entities\n",
    "    keys2keys['rel'] = 'rel' # relations\n",
    "    keys2keys['rel_eval'] = 'rel' # relations\n",
    "    keys2keys['e2'] = 'e1' # entities\n",
    "    keys2keys['e2_multi1'] = 'e1' # entity\n",
    "    keys2keys['e2_multi2'] = 'e1' # entity\n",
    "    input_keys = ['e1', 'rel', 'rel_eval', 'e2', 'e2_multi1', 'e2_multi2']\n",
    "    d = DatasetStreamer(input_keys)\n",
    "    d.add_stream_processor(JsonLoaderProcessors())\n",
    "    d.add_stream_processor(DictKey2ListMapper(input_keys))\n",
    "\n",
    "    # process full vocabulary and save it to disk\n",
    "    d.set_path(full_path)\n",
    "    p = Pipeline(args.data, delete_data, keys=input_keys, skip_transformation=True)\n",
    "    p.add_sent_processor(ToLower())\n",
    "    p.add_sent_processor(CustomTokenizer(lambda x: x.split(' ')),keys=['e2_multi1', 'e2_multi2'])\n",
    "    p.add_token_processor(AddToVocab())\n",
    "    p.execute(d)\n",
    "    p.save_vocabs()\n",
    "\n",
    "\n",
    "    # process train, dev and test sets and save them to hdf5\n",
    "    p.skip_transformation = False\n",
    "    for path, name in zip([train_path, dev_ranking_path, test_ranking_path], ['train', 'dev_ranking', 'test_ranking']):\n",
    "        d.set_path(path)\n",
    "        p.clear_processors()\n",
    "        p.add_sent_processor(ToLower())\n",
    "        p.add_sent_processor(CustomTokenizer(lambda x: x.split(' ')),keys=['e2_multi1', 'e2_multi2'])\n",
    "        p.add_post_processor(ConvertTokenToIdx(keys2keys=keys2keys), keys=['e1', 'rel', 'rel_eval', 'e2', 'e2_multi1', 'e2_multi2'])\n",
    "        p.add_post_processor(StreamToHDF5(name, samples_per_file=1000, keys=input_keys))\n",
    "        p.execute(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, model_path):\n",
    "    if args.preprocess:\n",
    "        preprocess(args.data, delete_data=True)\n",
    "    input_keys = ['e1', 'rel', 'rel_eval', 'e2', 'e2_multi1', 'e2_multi2']\n",
    "    p = Pipeline(args.data, keys=input_keys)\n",
    "    p.load_vocabs()\n",
    "    vocab = p.state['vocab']\n",
    "\n",
    "    train_batcher = StreamBatcher(args.data, 'train', args.batch_size, randomize=True, keys=input_keys, loader_threads=args.loader_threads)\n",
    "    dev_rank_batcher = StreamBatcher(args.data, 'dev_ranking', args.test_batch_size, randomize=False, loader_threads=args.loader_threads, keys=input_keys)\n",
    "    test_rank_batcher = StreamBatcher(args.data, 'test_ranking', args.test_batch_size, randomize=False, loader_threads=args.loader_threads, keys=input_keys)\n",
    "\n",
    "\n",
    "    data = []\n",
    "    rows = []\n",
    "    columns = []\n",
    "    num_entities = vocab['e1'].num_token\n",
    "    num_relations = vocab['rel'].num_token\n",
    "\n",
    "    if args.preprocess:\n",
    "        for i, str2var in enumerate(train_batcher):\n",
    "            print(\"batch number:\", i)\n",
    "            for j in range(str2var['e1'].shape[0]):\n",
    "                for k in range(str2var['e2_multi1'][j].shape[0]):\n",
    "                    if str2var['e2_multi1'][j][k] != 0:\n",
    "                        data.append(str2var['rel'][j].cpu().tolist()[0])\n",
    "                        rows.append(str2var['e1'][j].cpu().tolist()[0])\n",
    "                        columns.append(str2var['e2_multi1'][j][k].cpu().tolist())\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        from graph4nlp.pytorch.data.data import GraphData, to_batch\n",
    "        KG_graph = GraphData()\n",
    "        KG_graph.add_nodes(num_entities)\n",
    "        for e1, rel, e2 in zip(rows, data, columns):\n",
    "            KG_graph.add_edge(e1, e2)\n",
    "            eid = KG_graph.edge_ids(e1, e2)[0]\n",
    "            KG_graph.edge_attributes[eid]['token'] = rel\n",
    "\n",
    "        torch.save(KG_graph, '{}/processed/KG_graph.pt'.format(args.data))\n",
    "        return\n",
    "\n",
    "\n",
    "    if args.model is None:\n",
    "        model = ConvE(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'conve':\n",
    "        model = ConvE(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'distmult':\n",
    "        model = Distmult(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'complex':\n",
    "        model = Complex(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'ggnn_distmult':\n",
    "        model = GGNNDistMult(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'gcn_distmult':\n",
    "        model = GCNDistMult(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    elif args.model == 'gcn_complex':\n",
    "        model = GCNComplex(args, vocab['e1'].num_token, vocab['rel'].num_token)\n",
    "    else:\n",
    "        raise Exception(\"Unknown model!\")\n",
    "\n",
    "    if args.model in ['ggnn_distmult', 'gcn_distmult', 'gcn_complex']:\n",
    "        graph_path = '{}/processed/KG_graph.pt'.format(args.data)\n",
    "        KG_graph = torch.load(graph_path)\n",
    "        if Config.cuda:\n",
    "            KG_graph = KG_graph.to('cuda')\n",
    "    else:\n",
    "        KG_graph = None\n",
    "\n",
    "    train_batcher.at_batch_prepared_observers.insert(1,TargetIdx2MultiTarget(num_entities, 'e2_multi1', 'e2_multi1_binary'))\n",
    "\n",
    "    eta = ETAHook('train', print_every_x_batches=args.log_interval)\n",
    "    train_batcher.subscribe_to_events(eta)\n",
    "    train_batcher.subscribe_to_start_of_epoch_event(eta)\n",
    "    train_batcher.subscribe_to_events(LossHook('train', print_every_x_batches=args.log_interval))\n",
    "    if Config.cuda:\n",
    "        model.cuda()\n",
    "    if args.resume:\n",
    "        model_params = torch.load(model_path)\n",
    "        print(model)\n",
    "        total_param_size = []\n",
    "        params = [(key, value.size(), value.numel()) for key, value in model_params.items()]\n",
    "        for key, size, count in params:\n",
    "            total_param_size.append(count)\n",
    "            print(key, size, count)\n",
    "        print(np.array(total_param_size).sum())\n",
    "        model.load_state_dict(model_params)\n",
    "        model.eval()\n",
    "        ranking_and_hits(model, test_rank_batcher, vocab, 'test_evaluation', kg_graph=KG_graph)\n",
    "        ranking_and_hits(model, dev_rank_batcher, vocab, 'dev_evaluation', kg_graph=KG_graph)\n",
    "    else:\n",
    "        model.init()\n",
    "\n",
    "    total_param_size = []\n",
    "    params = [value.numel() for value in model.parameters()]\n",
    "    print(params)\n",
    "    print(np.sum(params))\n",
    "\n",
    "    best_mrr = 0\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        for i, str2var in enumerate(train_batcher):\n",
    "            opt.zero_grad()\n",
    "            e1 = str2var['e1']\n",
    "            rel = str2var['rel']\n",
    "            e2_multi = str2var['e2_multi1_binary'].float()\n",
    "            # label smoothing\n",
    "            e2_multi = ((1.0-args.label_smoothing)*e2_multi) + (1.0/e2_multi.size(1))\n",
    "\n",
    "            pred = model.forward(e1, rel, KG_graph)\n",
    "            loss = model.loss(pred, e2_multi)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_batcher.state.loss = loss.cpu()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if epoch % 2 == 0 and epoch > 0:\n",
    "                dev_mrr = ranking_and_hits(model, dev_rank_batcher, vocab, 'dev_evaluation', kg_graph=KG_graph)\n",
    "                if dev_mrr > best_mrr:\n",
    "                    best_mrr = dev_mrr\n",
    "                    print('saving best model to {0}'.format(model_path))\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            if epoch % 2 == 0:\n",
    "                if epoch > 0:\n",
    "                    ranking_and_hits(model, test_rank_batcher, vocab, 'test_evaluation', kg_graph=KG_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--kernel_size'], dest='kernel_size', nargs=None, const=None, default=5, type=<class 'int'>, choices=None, help='The side of the hidden layer. The required size changes with the size of the embeddings. Default: 9728 (embedding size 200).', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Link prediction for knowledge graphs')\n",
    "parser.add_argument('--batch-size', type=int, default=128, help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=128, help='input batch size for testing/validation (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=1000, help='number of epochs to train (default: 1000)')\n",
    "parser.add_argument('--lr', type=float, default=0.003, help='learning rate (default: 0.003)')\n",
    "parser.add_argument('--seed', type=int, default=1234, metavar='S', help='random seed (default: 17)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--data', type=str, default='kinship', help='Dataset to use: {FB15k-237, YAGO3-10, WN18RR, umls, nations, kinship}, default: FB15k-237')\n",
    "parser.add_argument('--l2', type=float, default=0.0, help='Weight decay value to use in the optimizer. Default: 0.0')\n",
    "parser.add_argument('--model', type=str, default='ggnn_distmult', help='Choose from: {conve, distmult, complex}')\n",
    "parser.add_argument('--direction_option', type=str, default='undirected', help='Choose from: {undirected, bi_sep, bi_fuse}')\n",
    "parser.add_argument('--embedding-dim', type=int, default=200, help='The embedding dimension (1D). Default: 200')\n",
    "parser.add_argument('--embedding-shape1', type=int, default=20, help='The first dimension of the reshaped 2D embedding. The second dimension is infered. Default: 20')\n",
    "parser.add_argument('--hidden-drop', type=float, default=0.25, help='Dropout for the hidden layer. Default: 0.3.')\n",
    "parser.add_argument('--input-drop', type=float, default=0.2, help='Dropout for the input embeddings. Default: 0.2.')\n",
    "parser.add_argument('--feat-drop', type=float, default=0.2, help='Dropout for the convolutional features. Default: 0.2.')\n",
    "parser.add_argument('--lr-decay', type=float, default=0.995, help='Decay the learning rate by this factor every epoch. Default: 0.995')\n",
    "parser.add_argument('--loader-threads', type=int, default=4, help='How many loader threads to use for the batch loaders. Default: 4')\n",
    "parser.add_argument('--preprocess', action='store_true', help='Preprocess the dataset. Needs to be executed only once. Default: 4')\n",
    "parser.add_argument('--resume', action='store_true', help='Resume a model.')\n",
    "parser.add_argument('--use-bias', action='store_true', help='Use a bias in the convolutional layer. Default: True')\n",
    "parser.add_argument('--label-smoothing', type=float, default=0.1, help='Label smoothing value to use. Default: 0.1')\n",
    "parser.add_argument('--hidden-size', type=int, default=9728, help='The side of the hidden layer. The required size changes with the size of the embeddings. Default: 9728 (embedding size 200).')\n",
    "\n",
    "parser.add_argument('--channels', type=int, default=200, help='The side of the hidden layer. The required size changes with the size of the embeddings. Default: 9728 (embedding size 200).')\n",
    "parser.add_argument('--kernel_size', type=int, default=5, help='The side of the hidden layer. The required size changes with the size of the embeddings. Default: 9728 (embedding size 200).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you run the task for the first time, run with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['--data', 'kinship', '--model', 'ggnn_distmult', '--preprocess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d8e1cd8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse console parameters and set global variables\n",
    "Config.backend = 'pytorch'\n",
    "Config.cuda = False\n",
    "Config.embedding_dim = args.embedding_dim\n",
    "\n",
    "model_name = '{2}_{0}_{1}'.format(args.input_drop, args.hidden_drop, args.model)\n",
    "model_path = 'saved_models/{0}_{1}.model'.format(args.data, model_name)\n",
    "\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After preprocess the kinship data, then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-29 09:15:44.238378 (WARNING): delete_all_previous_data=True! Deleting all folder contents of folder /Users/gaohanning/.data/kinship!\n",
      "2021-07-29 09:15:44.305367 (INFO): Recreating path: /Users/gaohanning/.data/kinship\n",
      "2021-07-29 09:15:44.525661 (INFO): Time taken for 10000 samples for input type rel for processor AddToVocab: 0.04 seconds\n",
      "2021-07-29 09:15:44.689667 (INFO): Time taken for 10000 samples for input type e2 for processor ToLower: 0.02 seconds\n",
      "2021-07-29 09:15:44.689906 (INFO): Time taken for 10000 samples for input type e2 for processor SaveLengthsToState: 0.04 seconds\n",
      "2021-07-29 09:15:44.910704 (INFO): Time taken for 10000 samples for input type e2_multi2 for processor CustomTokenizer: 0.02 seconds\n",
      "2021-07-29 09:15:44.963728 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab\n",
      "2021-07-29 09:15:44.965867 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_e1\n",
      "2021-07-29 09:15:44.966799 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_rel\n",
      "2021-07-29 09:15:44.967274 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_rel_eval\n",
      "2021-07-29 09:15:44.967722 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_e2\n",
      "2021-07-29 09:15:44.968158 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_e2_multi1\n",
      "2021-07-29 09:15:44.968574 (INFO): Saving vocab to: /Users/gaohanning/.data/kinship/vocab_e2_multi2\n",
      "2021-07-29 09:15:45.071121 (INFO): Time taken for 10000 samples for input type e2 for processor ToLower: 0.01 seconds\n",
      "2021-07-29 09:15:45.071301 (INFO): Time taken for 10000 samples for input type e2 for processor SaveLengthsToState: 0.03 seconds\n",
      "2021-07-29 09:15:45.448408 (INFO): Time taken for 10000 samples for input type e2 for processor ConvertTokenToIdx: 0.04 seconds\n",
      "2021-07-29 09:15:45.448675 (INFO): Time taken for 10000 samples for input type e2 for processor StreamToHDF5: 0.13 seconds\n",
      "2021-07-29 09:15:45.473820 (INFO): Time taken for 10000 samples for input type e2_multi2 for processor CustomTokenizer: 0.01 seconds\n",
      "2021-07-29 09:15:46.169435 (WARNING): Pipeline path /Users/gaohanning/.data/kinship already exist. This pipeline may overwrite data in this path!\n",
      "2021-07-29 09:15:46.171401 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab\n",
      "2021-07-29 09:15:46.173066 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e1\n",
      "2021-07-29 09:15:46.173688 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_rel\n",
      "2021-07-29 09:15:46.174137 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_rel_eval\n",
      "2021-07-29 09:15:46.174447 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2\n",
      "2021-07-29 09:15:46.174836 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2_multi1\n",
      "2021-07-29 09:15:46.175160 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2_multi2\n",
      "batch number: 0\n",
      "batch number: 1\n",
      "batch number: 2\n",
      "batch number: 3\n",
      "batch number: 4\n",
      "batch number: 5\n",
      "batch number: 6\n",
      "batch number: 7\n",
      "batch number: 8\n",
      "batch number: 9\n",
      "batch number: 10\n",
      "batch number: 11\n",
      "batch number: 12\n",
      "batch number: 13\n",
      "batch number: 14\n",
      "batch number: 15\n",
      "batch number: 16\n",
      "batch number: 17\n",
      "batch number: 18\n",
      "batch number: 19\n",
      "batch number: 20\n",
      "batch number: 21\n",
      "batch number: 22\n",
      "batch number: 23\n"
     ]
    }
   ],
   "source": [
    "main(args, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=['--data', 'kinship', '--model', 'ggnn_distmult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-29 09:17:46.109244 (WARNING): Pipeline path /Users/gaohanning/.data/kinship already exist. This pipeline may overwrite data in this path!\n",
      "2021-07-29 09:17:46.109869 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab\n",
      "2021-07-29 09:17:46.140476 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e1\n",
      "2021-07-29 09:17:46.189259 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_rel\n",
      "2021-07-29 09:17:46.190779 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_rel_eval\n",
      "2021-07-29 09:17:46.228481 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2\n",
      "2021-07-29 09:17:46.247424 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2_multi1\n",
      "2021-07-29 09:17:46.282809 (INFO): Loading vocab from: /Users/gaohanning/.data/kinship/vocab_e2_multi2\n",
      "[21200, 10400, 40000, 200, 120000, 120000, 600, 600]\n",
      "313000\n",
      "2021-07-29 09:18:10.399104 (INFO): Total epoch time: 0:00:16\n",
      "2021-07-29 09:18:10.422224 (INFO): \n",
      "\n",
      "2021-07-29 09:18:10.440596 (INFO): ########################################\n",
      "2021-07-29 09:18:10.477820 (INFO):           COMPLETED EPOCH: 1                              \n",
      "2021-07-29 09:18:10.478159 (INFO): train Loss: 0.69274\t99% CI: (0.69254, 0.69294), n=23\n",
      "2021-07-29 09:18:10.478188 (INFO): ########################################\n",
      "2021-07-29 09:18:10.478212 (INFO): \n",
      "\n",
      "2021-07-29 09:18:23.763029 (INFO): Total epoch time: 0:00:13\n",
      "2021-07-29 09:18:23.775885 (INFO): \n",
      "\n",
      "2021-07-29 09:18:23.775933 (INFO): ########################################\n",
      "2021-07-29 09:18:23.787836 (INFO):           COMPLETED EPOCH: 2                              \n",
      "2021-07-29 09:18:23.861385 (INFO): train Loss: 0.68472\t99% CI: (0.68181, 0.68764), n=23\n",
      "2021-07-29 09:18:23.861652 (INFO): ########################################\n",
      "2021-07-29 09:18:23.861686 (INFO): \n",
      "\n",
      "2021-07-29 09:18:36.978507 (INFO): Total epoch time: 0:00:13\n",
      "2021-07-29 09:18:36.979636 (INFO): \n",
      "\n",
      "2021-07-29 09:18:36.979678 (INFO): ########################################\n",
      "2021-07-29 09:18:36.979972 (INFO):           COMPLETED EPOCH: 3                              \n",
      "2021-07-29 09:18:36.980375 (INFO): train Loss: 0.60777\t99% CI: (0.58623, 0.6293), n=23\n",
      "2021-07-29 09:18:36.980405 (INFO): ########################################\n",
      "2021-07-29 09:18:36.980432 (INFO): \n",
      "\n",
      "2021-07-29 09:18:36.980591 (INFO): \n",
      "2021-07-29 09:18:36.980621 (INFO): --------------------------------------------------\n",
      "2021-07-29 09:18:36.980648 (INFO): dev_evaluation\n",
      "2021-07-29 09:18:36.980670 (INFO): --------------------------------------------------\n",
      "2021-07-29 09:18:36.980696 (INFO): \n",
      "2021-07-29 09:19:14.757078 (INFO): Hits left @1: 0.0107421875\n",
      "2021-07-29 09:19:14.761541 (INFO): Hits right @1: 0.0146484375\n",
      "2021-07-29 09:19:14.761695 (INFO): Hits @1: 0.0126953125\n",
      "2021-07-29 09:19:14.761791 (INFO): Hits left @2: 0.0244140625\n",
      "2021-07-29 09:19:14.761884 (INFO): Hits right @2: 0.02734375\n",
      "2021-07-29 09:19:14.762011 (INFO): Hits @2: 0.02587890625\n",
      "2021-07-29 09:19:14.762376 (INFO): Hits left @3: 0.041015625\n",
      "2021-07-29 09:19:14.774514 (INFO): Hits right @3: 0.03515625\n",
      "2021-07-29 09:19:14.806166 (INFO): Hits @3: 0.0380859375\n",
      "2021-07-29 09:19:14.818667 (INFO): Hits left @4: 0.05078125\n",
      "2021-07-29 09:19:14.818776 (INFO): Hits right @4: 0.0439453125\n",
      "2021-07-29 09:19:14.818903 (INFO): Hits @4: 0.04736328125\n",
      "2021-07-29 09:19:14.818993 (INFO): Hits left @5: 0.064453125\n",
      "2021-07-29 09:19:14.819083 (INFO): Hits right @5: 0.05859375\n",
      "2021-07-29 09:19:14.819204 (INFO): Hits @5: 0.0615234375\n",
      "2021-07-29 09:19:14.819291 (INFO): Hits left @6: 0.0869140625\n",
      "2021-07-29 09:19:14.819379 (INFO): Hits right @6: 0.0673828125\n",
      "2021-07-29 09:19:14.819712 (INFO): Hits @6: 0.0771484375\n",
      "2021-07-29 09:19:14.820349 (INFO): Hits left @7: 0.099609375\n",
      "2021-07-29 09:19:14.820449 (INFO): Hits right @7: 0.0810546875\n",
      "2021-07-29 09:19:14.820570 (INFO): Hits @7: 0.09033203125\n",
      "2021-07-29 09:19:14.820654 (INFO): Hits left @8: 0.1103515625\n",
      "2021-07-29 09:19:14.820735 (INFO): Hits right @8: 0.091796875\n",
      "2021-07-29 09:19:14.820858 (INFO): Hits @8: 0.10107421875\n",
      "2021-07-29 09:19:14.820938 (INFO): Hits left @9: 0.115234375\n",
      "2021-07-29 09:19:14.821017 (INFO): Hits right @9: 0.107421875\n",
      "2021-07-29 09:19:14.821136 (INFO): Hits @9: 0.111328125\n",
      "2021-07-29 09:19:14.821219 (INFO): Hits left @10: 0.125\n",
      "2021-07-29 09:19:14.821299 (INFO): Hits right @10: 0.125\n",
      "2021-07-29 09:19:14.821416 (INFO): Hits @10: 0.125\n",
      "2021-07-29 09:19:14.821546 (INFO): Mean rank left: 45.791015625\n",
      "2021-07-29 09:19:14.821897 (INFO): Mean rank right: 44.5009765625\n",
      "2021-07-29 09:19:14.822088 (INFO): Mean rank: 45.14599609375\n",
      "2021-07-29 09:19:14.822220 (INFO): Mean reciprocal rank left: 0.05987621308636564\n",
      "2021-07-29 09:19:14.822348 (INFO): Mean reciprocal rank right: 0.06136589750065763\n",
      "2021-07-29 09:19:14.828098 (INFO): Mean reciprocal rank: 0.06062105529351164\n",
      "saving best model to saved_models/kinship_ggnn_distmult_0.2_0.25.model\n",
      "2021-07-29 09:19:15.078124 (INFO): \n",
      "2021-07-29 09:19:15.078690 (INFO): --------------------------------------------------\n",
      "2021-07-29 09:19:15.078730 (INFO): test_evaluation\n",
      "2021-07-29 09:19:15.078757 (INFO): --------------------------------------------------\n",
      "2021-07-29 09:19:15.078780 (INFO): \n",
      "2021-07-29 09:19:56.407895 (INFO): Hits left @1: 0.0126953125\n",
      "2021-07-29 09:19:56.500230 (INFO): Hits right @1: 0.0107421875\n",
      "2021-07-29 09:19:56.500379 (INFO): Hits @1: 0.01171875\n",
      "2021-07-29 09:19:56.500473 (INFO): Hits left @2: 0.0244140625\n",
      "2021-07-29 09:19:56.500566 (INFO): Hits right @2: 0.0205078125\n",
      "2021-07-29 09:19:56.523864 (INFO): Hits @2: 0.0224609375\n",
      "2021-07-29 09:19:56.524303 (INFO): Hits left @3: 0.03515625\n",
      "2021-07-29 09:19:56.524402 (INFO): Hits right @3: 0.0341796875\n",
      "2021-07-29 09:19:56.524527 (INFO): Hits @3: 0.03466796875\n",
      "2021-07-29 09:19:56.524617 (INFO): Hits left @4: 0.0498046875\n",
      "2021-07-29 09:19:56.524706 (INFO): Hits right @4: 0.0458984375\n",
      "2021-07-29 09:19:56.524829 (INFO): Hits @4: 0.0478515625\n",
      "2021-07-29 09:19:56.524919 (INFO): Hits left @5: 0.05859375\n",
      "2021-07-29 09:19:56.525006 (INFO): Hits right @5: 0.0634765625\n",
      "2021-07-29 09:19:56.525127 (INFO): Hits @5: 0.06103515625\n",
      "2021-07-29 09:19:56.525216 (INFO): Hits left @6: 0.0703125\n",
      "2021-07-29 09:19:56.532243 (INFO): Hits right @6: 0.080078125\n",
      "2021-07-29 09:19:56.538576 (INFO): Hits @6: 0.0751953125\n",
      "2021-07-29 09:19:56.550240 (INFO): Hits left @7: 0.0927734375\n",
      "2021-07-29 09:19:56.550542 (INFO): Hits right @7: 0.0947265625\n",
      "2021-07-29 09:19:56.550675 (INFO): Hits @7: 0.09375\n",
      "2021-07-29 09:19:56.550786 (INFO): Hits left @8: 0.1015625\n",
      "2021-07-29 09:19:56.550880 (INFO): Hits right @8: 0.1025390625\n",
      "2021-07-29 09:19:56.551008 (INFO): Hits @8: 0.10205078125\n",
      "2021-07-29 09:19:56.551097 (INFO): Hits left @9: 0.1123046875\n",
      "2021-07-29 09:19:56.551184 (INFO): Hits right @9: 0.10546875\n",
      "2021-07-29 09:19:56.551305 (INFO): Hits @9: 0.10888671875\n",
      "2021-07-29 09:19:56.551449 (INFO): Hits left @10: 0.1220703125\n",
      "2021-07-29 09:19:56.551549 (INFO): Hits right @10: 0.115234375\n",
      "2021-07-29 09:19:56.551685 (INFO): Hits @10: 0.11865234375\n",
      "2021-07-29 09:19:56.551818 (INFO): Mean rank left: 45.4501953125\n",
      "2021-07-29 09:19:56.551947 (INFO): Mean rank right: 44.97265625\n",
      "2021-07-29 09:19:56.552143 (INFO): Mean rank: 45.21142578125\n",
      "2021-07-29 09:19:56.552274 (INFO): Mean reciprocal rank left: 0.06018809524834069\n",
      "2021-07-29 09:19:56.552401 (INFO): Mean reciprocal rank right: 0.058308364858043596\n",
      "2021-07-29 09:19:56.552600 (INFO): Mean reciprocal rank: 0.059248230053192144\n",
      "2021-07-29 09:20:08.650832 (INFO): Total epoch time: 0:00:12\n",
      "2021-07-29 09:20:08.652014 (INFO): \n",
      "\n",
      "2021-07-29 09:20:08.652057 (INFO): ########################################\n",
      "2021-07-29 09:20:08.657370 (INFO):           COMPLETED EPOCH: 4                              \n",
      "2021-07-29 09:20:08.657671 (INFO): train Loss: 0.35746\t99% CI: (0.3187, 0.39621), n=23\n",
      "2021-07-29 09:20:08.657705 (INFO): ########################################\n",
      "2021-07-29 09:20:08.658167 (INFO): \n",
      "\n",
      "2021-07-29 09:20:21.660291 (INFO): Total epoch time: 0:00:13\n",
      "2021-07-29 09:20:21.661538 (INFO): \n",
      "\n",
      "2021-07-29 09:20:21.661581 (INFO): ########################################\n",
      "2021-07-29 09:20:21.661608 (INFO):           COMPLETED EPOCH: 5                              \n",
      "2021-07-29 09:20:21.661630 (INFO): train Loss: 0.20784\t99% CI: (0.18635, 0.22933), n=23\n",
      "2021-07-29 09:20:21.716474 (INFO): ########################################\n",
      "2021-07-29 09:20:21.723205 (INFO): \n",
      "\n",
      "2021-07-29 09:20:21.729510 (INFO): \n",
      "2021-07-29 09:20:21.729834 (INFO): --------------------------------------------------\n",
      "2021-07-29 09:20:21.729868 (INFO): dev_evaluation\n",
      "2021-07-29 09:20:21.729895 (INFO): --------------------------------------------------\n",
      "2021-07-29 09:20:21.729921 (INFO): \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-29 09:20:55.339210 (INFO): Hits left @1: 0.0205078125\n",
      "2021-07-29 09:20:55.339673 (INFO): Hits right @1: 0.017578125\n",
      "2021-07-29 09:20:55.346089 (INFO): Hits @1: 0.01904296875\n",
      "2021-07-29 09:20:55.346916 (INFO): Hits left @2: 0.0419921875\n",
      "2021-07-29 09:20:55.352401 (INFO): Hits right @2: 0.0546875\n",
      "2021-07-29 09:20:55.383714 (INFO): Hits @2: 0.04833984375\n",
      "2021-07-29 09:20:55.390270 (INFO): Hits left @3: 0.0615234375\n",
      "2021-07-29 09:20:55.390377 (INFO): Hits right @3: 0.0791015625\n",
      "2021-07-29 09:20:55.390503 (INFO): Hits @3: 0.0703125\n",
      "2021-07-29 09:20:55.390596 (INFO): Hits left @4: 0.09765625\n",
      "2021-07-29 09:20:55.390685 (INFO): Hits right @4: 0.0986328125\n",
      "2021-07-29 09:20:55.390807 (INFO): Hits @4: 0.09814453125\n",
      "2021-07-29 09:20:55.390894 (INFO): Hits left @5: 0.1201171875\n",
      "2021-07-29 09:20:55.390982 (INFO): Hits right @5: 0.1123046875\n",
      "2021-07-29 09:20:55.391103 (INFO): Hits @5: 0.1162109375\n",
      "2021-07-29 09:20:55.391191 (INFO): Hits left @6: 0.1455078125\n",
      "2021-07-29 09:20:55.391565 (INFO): Hits right @6: 0.12890625\n",
      "2021-07-29 09:20:55.391693 (INFO): Hits @6: 0.13720703125\n",
      "2021-07-29 09:20:55.391805 (INFO): Hits left @7: 0.15625\n",
      "2021-07-29 09:20:55.391897 (INFO): Hits right @7: 0.1494140625\n",
      "2021-07-29 09:20:55.392028 (INFO): Hits @7: 0.15283203125\n",
      "2021-07-29 09:20:55.392113 (INFO): Hits left @8: 0.1767578125\n",
      "2021-07-29 09:20:55.392194 (INFO): Hits right @8: 0.1767578125\n",
      "2021-07-29 09:20:55.392309 (INFO): Hits @8: 0.1767578125\n",
      "2021-07-29 09:20:55.392389 (INFO): Hits left @9: 0.197265625\n",
      "2021-07-29 09:20:55.392469 (INFO): Hits right @9: 0.1962890625\n",
      "2021-07-29 09:20:55.392581 (INFO): Hits @9: 0.19677734375\n",
      "2021-07-29 09:20:55.392932 (INFO): Hits left @10: 0.212890625\n",
      "2021-07-29 09:20:55.393040 (INFO): Hits right @10: 0.2255859375\n",
      "2021-07-29 09:20:55.393164 (INFO): Hits @10: 0.21923828125\n",
      "2021-07-29 09:20:55.393311 (INFO): Mean rank left: 33.689453125\n",
      "2021-07-29 09:20:55.393441 (INFO): Mean rank right: 31.818359375\n",
      "2021-07-29 09:20:55.393624 (INFO): Mean rank: 32.75390625\n",
      "2021-07-29 09:20:55.393753 (INFO): Mean reciprocal rank left: 0.08981253771757543\n",
      "2021-07-29 09:20:55.393876 (INFO): Mean reciprocal rank right: 0.09296141486179754\n",
      "2021-07-29 09:20:55.394677 (INFO): Mean reciprocal rank: 0.09138697628968648\n",
      "saving best model to saved_models/kinship_ggnn_distmult_0.2_0.25.model\n",
      "2021-07-29 09:20:55.533230 (INFO): \n",
      "2021-07-29 09:20:55.533508 (INFO): --------------------------------------------------\n",
      "2021-07-29 09:20:55.533537 (INFO): test_evaluation\n",
      "2021-07-29 09:20:55.533566 (INFO): --------------------------------------------------\n",
      "2021-07-29 09:20:55.533597 (INFO): \n",
      "2021-07-29 09:21:29.736690 (INFO): Hits left @1: 0.021484375\n",
      "2021-07-29 09:21:29.768417 (INFO): Hits right @1: 0.021484375\n",
      "2021-07-29 09:21:29.768558 (INFO): Hits @1: 0.021484375\n",
      "2021-07-29 09:21:29.768666 (INFO): Hits left @2: 0.0458984375\n",
      "2021-07-29 09:21:29.768755 (INFO): Hits right @2: 0.037109375\n",
      "2021-07-29 09:21:29.768877 (INFO): Hits @2: 0.04150390625\n",
      "2021-07-29 09:21:29.775390 (INFO): Hits left @3: 0.0791015625\n",
      "2021-07-29 09:21:29.830495 (INFO): Hits right @3: 0.056640625\n",
      "2021-07-29 09:21:29.849420 (INFO): Hits @3: 0.06787109375\n",
      "2021-07-29 09:21:29.893433 (INFO): Hits left @4: 0.1025390625\n",
      "2021-07-29 09:21:29.893579 (INFO): Hits right @4: 0.0859375\n",
      "2021-07-29 09:21:29.893707 (INFO): Hits @4: 0.09423828125\n",
      "2021-07-29 09:21:29.893835 (INFO): Hits left @5: 0.123046875\n",
      "2021-07-29 09:21:29.893939 (INFO): Hits right @5: 0.1083984375\n",
      "2021-07-29 09:21:29.894081 (INFO): Hits @5: 0.11572265625\n",
      "2021-07-29 09:21:29.894171 (INFO): Hits left @6: 0.140625\n",
      "2021-07-29 09:21:29.894261 (INFO): Hits right @6: 0.12890625\n",
      "2021-07-29 09:21:29.894379 (INFO): Hits @6: 0.134765625\n",
      "2021-07-29 09:21:29.894463 (INFO): Hits left @7: 0.1572265625\n",
      "2021-07-29 09:21:29.894549 (INFO): Hits right @7: 0.1494140625\n",
      "2021-07-29 09:21:29.894669 (INFO): Hits @7: 0.1533203125\n",
      "2021-07-29 09:21:29.894754 (INFO): Hits left @8: 0.1826171875\n",
      "2021-07-29 09:21:29.895121 (INFO): Hits right @8: 0.173828125\n",
      "2021-07-29 09:21:29.895258 (INFO): Hits @8: 0.17822265625\n",
      "2021-07-29 09:21:29.895353 (INFO): Hits left @9: 0.2060546875\n",
      "2021-07-29 09:21:29.895455 (INFO): Hits right @9: 0.203125\n",
      "2021-07-29 09:21:29.895584 (INFO): Hits @9: 0.20458984375\n",
      "2021-07-29 09:21:29.895710 (INFO): Hits left @10: 0.2275390625\n",
      "2021-07-29 09:21:29.895807 (INFO): Hits right @10: 0.21875\n",
      "2021-07-29 09:21:29.896429 (INFO): Hits @10: 0.22314453125\n",
      "2021-07-29 09:21:29.896780 (INFO): Mean rank left: 33.537109375\n",
      "2021-07-29 09:21:29.896902 (INFO): Mean rank right: 33.1025390625\n",
      "2021-07-29 09:21:29.897095 (INFO): Mean rank: 33.31982421875\n",
      "2021-07-29 09:21:29.897226 (INFO): Mean reciprocal rank left: 0.09351879315949146\n",
      "2021-07-29 09:21:29.897738 (INFO): Mean reciprocal rank right: 0.08771723147725002\n",
      "2021-07-29 09:21:29.929262 (INFO): Mean reciprocal rank: 0.09061801231837074\n",
      "2021-07-29 09:21:42.805673 (INFO): Total epoch time: 0:00:12\n",
      "2021-07-29 09:21:42.848715 (INFO): \n",
      "\n",
      "2021-07-29 09:21:42.905165 (INFO): ########################################\n",
      "2021-07-29 09:21:42.992088 (INFO):           COMPLETED EPOCH: 6                              \n",
      "2021-07-29 09:21:43.071634 (INFO): train Loss: 0.21111\t99% CI: (0.18812, 0.23411), n=23\n",
      "2021-07-29 09:21:43.084255 (INFO): ########################################\n",
      "2021-07-29 09:21:43.084515 (INFO): \n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(args, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
